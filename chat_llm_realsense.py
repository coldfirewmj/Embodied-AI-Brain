import os
import sys
original_cwd = os.getcwd()
import time
from textwrap import dedent
import requests
from urllib.parse import urljoin
import json
from prompt_build import get_inst_plan,get_inst_chat,get_inst_find

import threading
import uvicorn
from fastapi import FastAPI, Request
import queue

# çº¿ç¨‹å®‰å…¨é˜Ÿåˆ—ï¼ˆæ¨èï¼‰
stt_queue = queue.Queue(maxsize=5)
# æ¥æ”¶STTæœåŠ¡å™¨çš„æ¶ˆæ¯
app = FastAPI(title="Main STT Receiver")

@app.post("/v1")
async def receive_stt_text(request: Request):
    try:
        body = await request.json()  # â† å¿…é¡» awaitï¼
        text = body.get("text", "")
        if text and text.strip():
            stt_queue.put(text.strip())
            print(f"ğŸ“¨ ä¸»è¿›ç¨‹æ”¶åˆ°STT: {text}")
        return {"status": "ok"}
    except Exception as e:
        print(f"âŒ è§£æJSONå¤±è´¥: {e}")
        return {"status": "error", "message": str(e)}

def start_stt_receiver():
    """åœ¨åå°çº¿ç¨‹å¯åŠ¨HTTPæ¥æ”¶æœåŠ¡"""
    uvicorn.run(app, host="127.0.0.1", port=28184, log_level="warning")
    print("STT receiver started."," host:","127.0.0.1"," port:",28184)

# å¯åŠ¨æ¥æ”¶æœåŠ¡ï¼ˆdaemon=True ç¡®ä¿éšä¸»è¿›ç¨‹é€€å‡ºï¼‰
threading.Thread(target=start_stt_receiver, daemon=True).start()

# TTSå‘é€ç»™æœåŠ¡å™¨
class TTSAgent:
    def __init__(self, host_url):
        print(f"TTSAgent: host_url {host_url}")
        self.host_url = host_url

    def run(self, input_dict: dict) -> str:
        # ç›´æ¥ä¼ å…¥å­—å…¸ï¼Œä¸è¦åœ¨å¤–é¢è½¬å­—ç¬¦ä¸²
        try:
            # è¿™é‡Œçš„è·¯å¾„ç›´æ¥æŒ‡å‘ /v1ï¼Œä¸ä½¿ç”¨ 'exec'
            resp = requests.post(self.host_url, json=input_dict, timeout=5)
            if resp.status_code == 200:
                return "ok"
        except Exception as e:
            print(f"TTSAgent Error: {e}")
        return ""

tts = TTSAgent("http://localhost:28185/v1")

def tts_sound(tts_agent, text, lang):
    # æ„é€ å­—å…¸
    input_dict = {"task": "text_to_speech", "lang": lang, "text": text}
    print(f"ğŸ“¤ å‘é€ä¸­: {text}")

    # ç›´æ¥å‘é€
    tts_agent.run(input_dict)
    return 0

# æµ‹è¯•è°ƒç”¨
# tts_sound(tts, "ä½ å¥½æˆ‘å«å°å¸…ï¼Œä½ ä¸€å®šå¬è¿‡æˆ‘çš„5åˆ†é’Ÿè®²ç”µå½±", "zh")
# exit()
# åˆ‡æ¢åˆ°sam2å·¥ä½œç›®å½•ï¼ŒåŠ è½½sam
print('å¼€å§‹åŠ è½½sam2æ¨¡å‹')
start_time = time.time()
sam_path = original_cwd+'/camera/Grounded-SAM-2'
sys.path.append(sam_path)
from sam2.build_sam import build_sam2
from sam2.sam2_image_predictor import SAM2ImagePredictor
from PIL import Image
CHECKPOINT_SAM = os.path.join(sam_path,"sam2_checkpoints", "sam2.1_hiera_small.pt")
# CONFIG_SAMå‰é¢å¿…é¡»åŠ '/'
CONFIG_SAM = '/'+sam_path+"/sam2/configs/sam2.1/sam2.1_hiera_s.yaml"
# Load the model
sam2_model = build_sam2(CONFIG_SAM, CHECKPOINT_SAM, device="cuda")
sam_predictor = SAM2ImagePredictor(sam2_model)
print(f"ğŸ‰ åŠ è½½SAMæ¨¡å‹è€—æ—¶: {time.time() - start_time:.2f} ç§’")

# åŠ è½½realsenseå·¥ä½œç›®å½•
print('å¼€å§‹åŠ è½½realsenseå·¥ä½œç›®å½•')
import pyrealsense2 as rs
import cv2
import numpy as np
# D455ç›¸æœºåˆå§‹åŒ–
pipeline = rs.pipeline()
config = rs.config()
# é…ç½®å½©è‰²æµï¼ˆSAM3å¤„ç†RGBå›¾åƒï¼‰ï¼šåˆ†è¾¨ç‡640x480ï¼Œå¸§ç‡30
config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)
config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)
align = rs.align(rs.stream.color)
pipeline.start(config)

# å¯åŠ¨å¤§æ¨¡å‹å®¢æˆ·ç«¯
print('å¼€å§‹è¿æ¥å¤§æ¨¡å‹æœåŠ¡')
start_time= time.time()
import base64
from openai import OpenAI
client = OpenAI(base_url="http://localhost:8000/v1", api_key="EMPTY")
print(f"ğŸ‰ åŠ è½½å¤§æ¨¡å‹æœåŠ¡è€—æ—¶: {time.time() - start_time:.2f} ç§’")
start_time= time.time()

# åˆæˆmessages
def input_messages_build(prompt, image=None):
    print(prompt)
    # æ„å»ºæ–‡æœ¬ä¿¡æ¯
    text_content = {
        "type": "text",
        "text": prompt
    }
    if image is None:
        return [
        {
            "role": "user",
            "content": [text_content],
        }]
    else:
        # --- ä¿®æ”¹éƒ¨åˆ†ï¼šå…ˆç¼–ç ä¸º png æ ¼å¼ ---
        # image æ˜¯ä» realsense è·å–çš„ BGR æ•°ç»„
        success, buffer = cv2.imencode('.png', image)
        if not success:
            raise ValueError("æ— æ³•ç¼–ç å›¾åƒ")

        # è½¬æ¢ä¸º base64
        image_b64 = base64.b64encode(buffer).decode("utf-8")
        # æ„å»ºå›¾åƒä¿¡æ¯
        image_contents = {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/png;base64,{image_b64}"
                    },
                }
        return [
            {
                "role": "user",
                "content":   [text_content, image_contents],
            }]

import re
sentence_endings = r'[ã€‚ï¼ï¼Ÿ!?ï¼›;â€¦\n]'  # ä¸­è‹±æ–‡å¥å°¾ç¬¦å·
# å¾—åˆ°å›ç­”å¹¶è§£æå‡ºåŒ…å›´æ¡†
def get_respose_(model,text,image):
    start_time = time.time()
    # æ­¤å¤„åšä¸€ä¸ªåˆ¤æ–­ï¼Œå¦‚æœæ˜¯Cåˆ™èŠå¤©ï¼Œå¦‚æœæ˜¯Gåˆ™æŠ“å–ï¼Œç”±äºæ¥ä¸‹æ¥å¦‚æœæ˜¯Cåˆ™æµå¼è¾“å‡ºçš„ï¼Œæ‰€ä»¥éœ€è¦å…ˆåˆ¤æ–­
    prompt = get_inst_plan(text)
    messages = input_messages_build(prompt)
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=0.2,
        extra_body={"mm_processor_kwargs":{"fps": [1]}},
        stream=False,
    )
    choices = response.choices[0].message.content
    print("é€‰æ‹©çš„æ“ä½œç±»å‹ï¼š", choices)
    print(f"é€‰æ‹©æ“ä½œè€—æ—¶: {time.time()-start_time}")
    start_time = time.time()
    if choices == 'C':
        prompt = get_inst_chat(text)
        messages = input_messages_build(prompt, image)
        response = client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=0.2,
            extra_body={"mm_processor_kwargs":{"fps": [1]}},
            stream=True,
        )
        # sentence = response.choices[0].message.content
        # tts_sound(tts,sentence,"zh")
        buffer = ""
        for chunk in response:
            if chunk.choices and chunk.choices[0].delta.content:
                token = chunk.choices[0].delta.content
                buffer += token
                # æ£€æŸ¥æ˜¯å¦é‡åˆ°å®Œæ•´å¥å­ç»“å°¾
                if re.search(sentence_endings, token):
                    # å»æ‰æœ«å°¾ç©ºç™½
                    sentence = buffer.strip()
                    if sentence:
                        tts_sound(tts,sentence,"zh")
                        buffer = ""  # æ¸…ç©ºç¼“å†²åŒº

        # å¤„ç†æœ€åä¸€å¥ï¼ˆå¦‚æœæ²¡ä»¥å¥å·ç»“å°¾ï¼‰
        if buffer.strip():
            sentence = buffer.strip()
            tts_sound(tts,sentence,"zh")
        entity = []
    elif choices == 'G':
        prompt = get_inst_find(text)
        messages = input_messages_build(prompt, image)
        response = client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=0.2,
            extra_body={"mm_processor_kwargs":{"fps": [1]}},
            stream=False,
        )
        chunk = response.choices[0].message.content
        print("å›å¤ä¸ºï¼š",chunk)
        print("å›å¤ä¸ºï¼š",chunk)
        if chunk is None or chunk == '[]':
            tts_sound(tts,"æœªæ‰¾åˆ°ç‰©ä½“","zh")
            entity = []
            return entity
        entity = json.loads(chunk)
        print('åŒ…å›´æ¡†ä¸ºï¼š',entity)
        tts_sound(tts,"å·²æ‰¾åˆ°ç‰©ä½“","zh")
    print(f"å†æ¬¡åˆ¤æ–­è€—æ—¶: {time.time()-start_time}")
    # entity = stream_message.strip('```json\n').strip('```')
    # print("entityï¼š", entity)
    return entity

def main():
    out_image = None
    while True:
        frames = pipeline.wait_for_frames()
        aligned_frames = align.process(frames)
        color_frame = aligned_frames.get_color_frame()
        depth_frame = aligned_frames.get_depth_frame()
        if not color_frame or not depth_frame :
            continue
        image = np.asanyarray(color_frame.get_data())
        depth_image = np.asanyarray(depth_frame.get_data())
        # åç»­å¯å®‰å…¨åœ°ç”¨æ­¤ è½¬æ¢ä¸ºç‚¹äº‘
        depth_intrin = depth_frame.profile.as_video_stream_profile().get_intrinsics()

        try:
            text = stt_queue.get(timeout=0.01)
            # è·å–åŒ…å›´æ¡†
            input_box = get_respose_('Qwen2.5-VL-7B-Instruct',text,image)
            if input_box!=[]:
                start_time = time.time()
                x1, y1, x2, y2 = input_box
                # è½¬æ¢ä¸ºRGBæ ¼å¼
                img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # SAM3éœ€è¦RGBæ ¼å¼
                # -------------------------- SAM3æ¨ç† --------------------------
                # Load an image
                img_pil = Image.fromarray(img_rgb)
                print("+++++++++++++++++++++++++++++++++++++++++++++++++++++++++")
                inference_state = sam_predictor.set_image(img_pil)
                print("=========================================================")
                masks, scores, _ = sam_predictor.predict(
                    box=input_box,
                    multimask_output=False  # å•maskæ›´é«˜æ•ˆ
                )
                print("æ¡†æ©ç é¢„æµ‹æ—¶é—´ä¸º",time.time()-start_time)
                print("å¯è§†åŒ–")
                mask = masks[np.argmax(scores)]
                # æ©ç è½¬äºŒå€¼å›¾
                mask_np = mask.astype(np.uint8) * 255
                # æ‰¾æ©ç è½®å»“
                contours, _ = cv2.findContours(mask_np, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                print("è½®å»“æ•°é‡ï¼š", len(contours))
                # ç»˜åˆ¶è½®å»“ï¼ˆçº¢è‰²ï¼Œçº¿å®½2ï¼‰
                cv2.drawContours(image, contours, -1, (0, 0, 255), 2)
                # ç»˜åˆ¶åŒ…å›´æ¡†ï¼ˆç»¿è‰²ï¼Œçº¿å®½2ï¼‰
                cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)
                # è·å¾—è¯¥ç‰©ä½“çš„xyzåæ ‡ï¼ˆåŒ…å›´æ¡†å†…æ©ç çš„åƒç´ ç‚¹å¯¹åº”çš„xyzæ±‚å¹³å‡ï¼‰
                for u in range(x1,x2+1):
                    for v in range(y1,y2+1):
                        depth_val = depth_image[v,u]
                        if(depth_val > 0 and mask[v,u]):
                            [x,y,z] = rs.rs2_deproject_pixel_to_point(
                                depth_intrin, [u, v], depth_val / 1000.0)
                            # print('ç‰©ä½“åæ ‡æ˜¯ï¼šx:',x,'y:',y,'z:',z)
                            break
                out_image = image.copy()
        except queue.Empty:
            # å½“é˜Ÿåˆ—ä¸ºç©ºæ—¶ï¼Œç»§ç»­å¾ªç¯è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
            pass

        # æ˜¾ç¤ºç»“æœ
        if out_image is not None:
            cv2.imshow("SAM3 mask in box", out_image)

            key = cv2.waitKey(1)& 0xFF
            # æŒ‰qæˆ–è€…ESCé€€å‡º
            if key == ord('q') or key == 27:
                break

if __name__ == "__main__":
        main()
