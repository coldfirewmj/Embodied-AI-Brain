import os
import sys
original_cwd = os.getcwd()
import time
from textwrap import dedent
import threading
import requests
from urllib.parse import urljoin
# ç”¨äºæ§åˆ¶å½•éŸ³çº¿ç¨‹
recording_lock = threading.Lock()
# æ ‡è®°å½“å‰æ˜¯å¦å…è®¸å½•éŸ³
# åˆ›å»ºå½•éŸ³+è¯†åˆ«å™¨ï¼ˆå…³é”®é…ç½®ï¼‰
import sounddevice as sd
# è·å–éŸ³é¢‘è®¾å¤‡çš„ç¼–å·
def get_in_sounddevice_index(target_name='PnP'):
    devices = sd.query_devices()
    for idx, device in enumerate(devices):
        if target_name in device['name'] and device['max_input_channels'] > 0:
            print("éº¦å…‹é£:",device['name'],' index:',idx)
            return idx
    return None
input_sound_index = get_in_sounddevice_index()
def get_out_ounddevice_index(target_name='default'):
    devices = sd.query_devices()
    for idx, device in enumerate(devices):
        if target_name in device['name'] and device['max_output_channels'] > 0:
            print("æ‰¬å£°å™¨:",device['name'],' index:',idx)
            return idx
    return None
output_device_index = get_out_ounddevice_index()

class TTSAgent:

    def __init__(self, host_url):
        print(f"TTSAgent: host_url {host_url}")
        self.host_url = host_url

    def run(self, input_dict_str: str) -> str:
        data = {"task": input_dict_str}
        try:
            start_time = time.time()
            resp = requests.post(urljoin(self.host_url, 'exec'), data=data, timeout=10)
            duration = time.time() - start_time
            #print(f"TTSAgent: post_duration {duration:.3f}")
            try:
                resp_dict = json.loads(resp.text)
                out_text = resp_dict['out_text']
                #print(f"Recv: out_text {out_text}")
            except:
                out_text = ""
        except requests.exceptions.Timeout as e:
            print('TTSAgent: Timeout')
            out_text = ""
        return out_text

def create_tts_agent(host_url: str = None):
    host_url = host_url or os.getenv('RABBITBOT_TTS_AGENT_URL', 'http://127.0.0.1:8001')
    return TTSAgent(host_url)

tts = create_tts_agent("http://localhost:28185/v1")

# åŠ è½½whisperæ¨¡å‹
sys.path.append(original_cwd+'/audio/RealTimeSTT')
from RealtimeSTT import AudioToTextRecorder
stt_path = original_cwd+"/Models/faster-whisper-large-v3-turbo"
print('å¼€å§‹whisperåŠ è½½æ¨¡å‹')
start_time = time.time()
WAKE_WORD = "ä½ å¥½å°åŸŸ"
SENSITIVITY = 0.5
recorder = AudioToTextRecorder(
    # æ¨¡å‹å¤§å°ï¼š/base
    model=stt_path,
    silero_vad_path= original_cwd+'/Models/snakers4_silero-vad_master',
    # å¼ºåˆ¶ä¸­æ–‡ï¼ˆæé«˜å‡†ç¡®ç‡ï¼‰
    language="zh",
    compute_type="float16",
    device="cuda",
    # æ˜¯å¦ä½¿ç”¨éº¦å…‹é£è¾“å…¥ï¼ŒFalseè¡¨ç¤ºä½¿ç”¨å›è°ƒå‡½æ•°è¾“å…¥
    use_microphone=True,
    # æ·»åŠ å”¤é†’è¯
    # wake_words=WAKE_WORD,
    # wake_words_sensitivity=SENSITIVITY,
    initial_prompt="ä»¥ä¸‹æ˜¯æ™®é€šè¯çš„å¥å­ã€‚",
    # å¦‚æœä½ çŸ¥é“éº¦å…‹é£è®¾å¤‡ç´¢å¼•ï¼Œå–æ¶ˆæ³¨é‡Šä¸‹ä¸€è¡Œï¼š
    input_device_index=input_sound_index,  # æ›¿æ¢ä¸ºä½ çš„éº¦å…‹é£ç´¢å¼•ï¼ˆé€šè¿‡ sounddevice_devices.py è·å–ï¼‰
)
print(f"ğŸ‰ åŠ è½½whisperæ¨¡å‹è€—æ—¶: {time.time() - start_time:.2f} ç§’")
# è¿‡æ»¤æ–‡æœ¬
def preprocess_voice_text(text):
    if not text:
        return None
    text = text.strip()
    if not text or text.isspace():
        return None
    text = re.sub(r"^(å—¯|å•Š|å“¦|å‘ƒ|å“)\s*", "", text)
    text = text[:2000]
    print(text)
    return text
start_time= time.time()

# åˆ‡æ¢åˆ°sam2å·¥ä½œç›®å½•ï¼ŒåŠ è½½sam
print('å¼€å§‹åŠ è½½sam2æ¨¡å‹')
sam_path = original_cwd+'/vision/sam2'
sys.path.append(sam_path)
from sam2.build_sam import build_sam2
from sam2.sam2_image_predictor import SAM2ImagePredictor
import re
from PIL import Image
CHECKPOINT_SAM = os.path.join(sam_path,"sam2_checkpoints", "sam2.1_hiera_small.pt")
# CONFIG_SAMå‰é¢å¿…é¡»åŠ '/'
CONFIG_SAM = '/'+sam_path+"/sam2/configs/sam2.1/sam2.1_hiera_s.yaml"
# Load the model
sam2_model = build_sam2(CONFIG_SAM, CHECKPOINT_SAM, device="cuda")
sam_predictor = SAM2ImagePredictor(sam2_model)
print(f"ğŸ‰ åŠ è½½SAMæ¨¡å‹è€—æ—¶: {time.time() - start_time:.2f} ç§’")

# åŠ è½½realsenseå·¥ä½œç›®å½•
print('å¼€å§‹åŠ è½½realsenseå·¥ä½œç›®å½•')
import pyrealsense2 as rs
import cv2
import numpy as np
# D455ç›¸æœºåˆå§‹åŒ–
pipeline = rs.pipeline()
config = rs.config()
# é…ç½®å½©è‰²æµï¼ˆSAM3å¤„ç†RGBå›¾åƒï¼‰ï¼šåˆ†è¾¨ç‡640x480ï¼Œå¸§ç‡30
config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)
config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)
align = rs.align(rs.stream.color)
pipeline.start(config)

# å¯åŠ¨å¤§æ¨¡å‹å®¢æˆ·ç«¯
print('å¼€å§‹è¿æ¥å¤§æ¨¡å‹æœåŠ¡')
start_time= time.time()
import base64
from openai import OpenAI
client = OpenAI(base_url="http://localhost:8000/v1", api_key="EMPTY")
print(f"ğŸ‰ åŠ è½½å¤§æ¨¡å‹æœåŠ¡è€—æ—¶: {time.time() - start_time:.2f} ç§’")
start_time= time.time()

# åˆæˆmessages
def box_messages_build(prompt, image):
    # --- ä¿®æ”¹éƒ¨åˆ†ï¼šå…ˆç¼–ç ä¸º png æ ¼å¼ ---
    # image æ˜¯ä» realsense è·å–çš„ BGR æ•°ç»„
    success, buffer = cv2.imencode('.png', image)
    if not success:
        raise ValueError("æ— æ³•ç¼–ç å›¾åƒ")

    # è½¬æ¢ä¸º base64
    image_b64 = base64.b64encode(buffer).decode("utf-8")
    # æ„å»ºå›¾åƒä¿¡æ¯
    image_contents = {
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/png;base64,{image_b64}"
                },
            }
    # æ„å»ºæ–‡æœ¬ä¿¡æ¯
    text_content = {
        "type": "text",
        "text": prompt
    }
    return [
    {"role": "system", "content":
    "You must output ONLY a JSON array of four integers: [x1, y1, x2, y2]. No explanation. No markdown. Just the array."},
        {
            "role": "user",
            "content":   [text_content, image_contents],
        }]
# å¾—åˆ°å›ç­”å¹¶è§£æå‡ºåŒ…å›´æ¡†
def get_respose_box(model,messages):
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=0.2,
        extra_body={"mm_processor_kwargs":{"fps": [1]}},
        stream=False,
    )

    entity = response.choices[0].message.content.strip('```json\n').strip('```')
    # stream_message = ""
    # for chunk in response:
    #     try:
    #         stream_message += chunk.choices[0].delta.content
    #         print(chunk.choices[0].delta.content, end='')
    #     except AttributeError as e:
    #         if "'str' object has no attribute 'choices'" in str(e):
    #             pass
    #         else:
    #             raise
    # entity = stream_message.strip('```json\n').strip('```')
    print("entityï¼š", entity)
    entity_bbox = entity
    print("test is exec!!!!!!!", entity_bbox)
    return entity_bbox
import json
def main():
    while True:
        frames = pipeline.wait_for_frames()
        aligned_frames = align.process(frames)
        color_frame = aligned_frames.get_color_frame()
        depth_frame = aligned_frames.get_depth_frame()
        if not color_frame or not depth_frame :
            continue
        image = np.asanyarray(color_frame.get_data())
        depth_image = np.asanyarray(depth_frame.get_data())
        text = preprocess_voice_text(recorder.text())
        prompt = dedent(f"""\
            ä½ æ˜¯ä¸€åè¯†åˆ«ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä»æœºå™¨äººè§†è§’ï¼Œä»…æ ¹æ®ç”¨æˆ·**æŒ‡å®šç‰©ä½“**çš„ä½ç½®ã€‚
            è¯·é¦–å…ˆåˆ¤æ–­ç‰©ä½“æ˜¯å¦åœ¨ç”»é¢å†…ï¼Œç„¶åè¾“å‡ºå…¶åŒ…å›´ç›’ï¼š

            **è¾“å‡ºè¦æ±‚ï¼š**
            è¯·ç›´æ¥è¾“å‡ºè¯†åˆ«åˆ°çš„ç”¨æˆ·è¦æ±‚çš„ç‰©ä½“ä¿¡æ¯â€œ{text}â€çš„åŒ…å›´æ¡†ï¼Œè¯·ä»”ç»†åˆ†è¾¨ç‰©ä½“çš„å½¢çŠ¶å’Œé¢œè‰²ï¼Œä¸è¦åŒ…å«ä»»ä½•æ–‡å­—ä¿¡æ¯ï¼ŒåŒ…æ‹¬plaintextã€‚
            """)
        # prompt = dedent(""" è¯·å›ç­”ä½ æ˜¯è°""")
        # åç»­å¯å®‰å…¨åœ°ç”¨æ­¤ è½¬æ¢ä¸ºç‚¹äº‘
        depth_intrin = depth_frame.profile.as_video_stream_profile().get_intrinsics()
        # è·å–åŒ…å›´æ¡†
        entity_bbox = get_respose_box('Qwen2.5-VL-7B-Instruct',box_messages_build(prompt,image))
        if entity_bbox!=[]:
            # input_box = list(map(int, entity_bbox[0].split(',')))
            input_box = json.loads(entity_bbox)
            x1, y1, x2, y2 = input_box
            input_box = np.array(input_box)
            print(input_box)
            # è½¬æ¢ä¸ºRGBæ ¼å¼
            img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # SAM3éœ€è¦RGBæ ¼å¼
            # -------------------------- SAM3æ¨ç† --------------------------
            # Load an image
            img_pil = Image.fromarray(img_rgb)
            inference_state = sam_predictor.set_image(img_pil)
            masks, scores, _ = sam_predictor.predict(
                box=input_box,
                multimask_output=False  # å•maskæ›´é«˜æ•ˆ
            )
            print("æ¡†æ©ç é¢„æµ‹æ—¶é—´ä¸º",time.time()-start_time)
            print("å¯è§†åŒ–")
            mask = masks[np.argmax(scores)]
            # æ©ç è½¬äºŒå€¼å›¾
            mask_np = mask.astype(np.uint8) * 255
            # æ‰¾æ©ç è½®å»“
            contours, _ = cv2.findContours(mask_np, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            print("è½®å»“æ•°é‡ï¼š", len(contours))
            # ç»˜åˆ¶è½®å»“ï¼ˆçº¢è‰²ï¼Œçº¿å®½2ï¼‰
            cv2.drawContours(image, contours, -1, (0, 0, 255), 2)
            # ç»˜åˆ¶åŒ…å›´æ¡†ï¼ˆç»¿è‰²ï¼Œçº¿å®½2ï¼‰
            cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)
            # è·å¾—è¯¥ç‰©ä½“çš„xyzåæ ‡ï¼ˆåŒ…å›´æ¡†å†…æ©ç çš„åƒç´ ç‚¹å¯¹åº”çš„xyzæ±‚å¹³å‡ï¼‰
            # for u in range(x1,x2+1):
            #     for v in range(y1,y2+1):
            #         depth_val = depth_image[v,u]
            #         if(depth_val > 0 and mask[v,u]):
            #             [x,y,z] = rs.rs2_deproject_pixel_to_point(
            #                 depth_intrin, [u, v], depth_val / 1000.0)
            #             # print('ç‰©ä½“åæ ‡æ˜¯ï¼šx:',x,'y:',y,'z:',z)
            #             break

        out_image = image.copy()
        # æ˜¾ç¤ºç»“æœ
        sentence = 'ç‰©ä½“å·²æ˜¾ç¤º'
        cv2.imshow("SAM3 mask in box", out_image)

        key = cv2.waitKey(1)& 0xFF
        # æŒ‰qæˆ–è€…ESCé€€å‡º
        if key == ord('q') or key == 27:
            break

if __name__ == "__main__":
        main()
