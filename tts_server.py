import os
import sys
import time
import threading
import heapq
import queue
import numpy as np
import torch
import sounddevice as sd
from scipy.signal import resample_poly
import uvicorn
from fastapi import FastAPI, Request

# ===== é…ç½® =====
KOKORO_OFFICIAL_SR = 24000.0
TARGET_SR = 44100
VOICE_NAME = "zm_yunxi"
os.environ["HF_HUB_OFFLINE"] = "1"
# ===== æ‰¬å£°å™¨æŸ¥æ‰¾ =====
def get_output_device_index(target_name='default'):
    devices = sd.query_devices()
    for idx, dev in enumerate(devices):
        if target_name in dev['name'] and dev['max_output_channels'] > 0:
            return idx
    return None

output_sound_index = get_output_device_index()
print('output_sound_index is:',output_sound_index)

start_time = time.time()
# æ¨¡å‹åŠ è½½éƒ¨åˆ† (ä¿æŒåŸæ ·)
original_cwd = os.getcwd()
sys.path.append(os.path.join(original_cwd, 'audio', 'kokoro'))
from kokoro import KPipeline, KModel
model = KModel(config=original_cwd + "/Models/Kokoro-82M/config.json",
              model=original_cwd + "/Models/Kokoro-82M/kokoro-v1_0.pth")
model = model.to('cuda').eval()
pipeline = KPipeline(lang_code='z', device="cuda", model=model)

# ===== å…¨å±€çŠ¶æ€ç®¡ç† =====
# éŸ³é¢‘å †ï¼šå­˜æ”¾æ¨ç†å®Œæˆå¾…æ’­æ”¾çš„éŸ³é¢‘ (sid, seq_id, audio_data)
raw_audio_heap = []

current_session_id = 0
expected_seq_id = 0
recive_seq_id = 0

# æ ¸å¿ƒåŒæ­¥é”ï¼šä¿æŠ¤æ‰€æœ‰å…¨å±€å˜é‡å’ŒéŸ³é¢‘è®¾å¤‡æ“ä½œ
GLOBAL_LOCK = threading.Lock()
# æ‰“æ–­ä¿¡å·
interrupt_event = threading.Event()

# ===== 1. ç»Ÿä¸€å¤„ç†çº¿ç¨‹ (Producer & Consumer åè°ƒ) =====

def tts_manager_worker():
    global expected_seq_id, current_session_id, start_time

    # åˆ›å»ºä¸€ä¸ªæŒä¹…çš„è¾“å‡ºæµ
    # samplerate=TARGET_SR, channels=1 (å•å£°é“)
    stream = sd.OutputStream(samplerate=TARGET_SR, channels=1, dtype='float32')
    stream.start()
    # å®šä¹‰æ¯å—å†™å…¥çš„å¤§å° (æ¯”å¦‚ 1024 å¸§ï¼Œçº¦ 23ms)
    CHUNK_SIZE = 1024
    print("ğŸš€ å¼‚æ­¥æ’­æ”¾æµå·²å¯åŠ¨...")

    while True:
        audio_to_play = None
        this_sid = -1

        with GLOBAL_LOCK:
            if raw_audio_heap:
                top_sid, top_seq, audio_data = raw_audio_heap[0]

                # A: ä¸¢å¼ƒè¿‡æ—¶æ•°æ®
                if top_sid < current_session_id:
                    heapq.heappop(raw_audio_heap)
                    continue

                # B: åŒ¹é…å½“å‰åºå·
                if top_sid == current_session_id and top_seq == expected_seq_id:
                    _, _, audio_to_play = heapq.heappop(raw_audio_heap)
                    this_sid = top_sid

        if audio_to_play is not None:
            # --- å…³é”®ä¿®æ”¹ï¼šåˆ‡ç‰‡å¼å†™å…¥ ---
            # å°† numpy æ•°ç»„æŒ‰ CHUNK_SIZE åˆ‡åˆ†
            num_samples = len(audio_to_play)
            print(F'é¦–å¥TTSæ—¶é—´ä¸ºï¼š{time.time() - start_time}s')
            for i in range(0, num_samples, CHUNK_SIZE):
                # æ¯ä¸€å°å—å†™å…¥å‰ï¼Œéƒ½æ£€æŸ¥ä¸€æ¬¡æ‰“æ–­ä¿¡å·
                if interrupt_event.is_set() or this_sid != current_session_id:
                    print(f"ğŸ›‘ ç‰©ç†æ‰“æ–­æ‰§è¡Œï¼šä¸¢å¼ƒ Session {this_sid} å‰©ä½™éŸ³é¢‘")
                    stream.stop()  # ç«‹å³æ¸…ç©ºå£°å¡ç¼“å†²åŒº
                    stream.start() # é‡æ–°å¯åŠ¨æµå‡†å¤‡æ¥æ”¶æ–°å£°
                    break

                chunk = audio_to_play[i : i + CHUNK_SIZE]
                # å¦‚æœæœ€åä¸€å—ä¸å¤Ÿå¤§ï¼Œè¡¥é½å®ƒæˆ–è€…ç›´æ¥å†™
                stream.write(chunk)
            else:
                # åªæœ‰å®Œæ•´æ’­å®Œï¼ˆæ²¡æœ‰è¢« breakï¼‰ï¼Œæ‰å¢åŠ åºå·
                with GLOBAL_LOCK:
                    expected_seq_id += 1
        else:
            # å¦‚æœæ²¡æœ‰éŸ³é¢‘ï¼Œä¸”å½“å‰å¤„äºæ‰“æ–­çŠ¶æ€ï¼Œå†æ¬¡ç¡®ä¿æµæ˜¯ç©ºçš„
            if interrupt_event.is_set():
                stream.stop()
                stream.start()
            time.sleep(0.01)
# å¯åŠ¨ç®¡ç†çº¿ç¨‹
threading.Thread(target=tts_manager_worker, daemon=True).start()

# ===== 2. æ¥å£å±‚ =====

app = FastAPI()

@app.post("/v1")
async def receive_tts_text(request: Request):
    global current_session_id, expected_seq_id, recive_seq_id, start_time

    body = await request.json()
    text = body.get("text", "").strip()
    is_new_talk = bool(body.get("interrupt", False))

    print(f"æ”¶åˆ°è¯·æ±‚ï¼štext='{text}', interrupt={is_new_talk}")
    if not text: return {"status": "empty"}

    # 1. å¤„ç†æ‰“æ–­é€»è¾‘ (è¿™éƒ¨åˆ†å¿…é¡»åŠ é”)
    with GLOBAL_LOCK:
        if is_new_talk:
            start_time = time.time()
            interrupt_event.set()
            # è¿™é‡Œä¸éœ€è¦ä¸“é—¨è°ƒç”¨ sd.stop() äº†ï¼Œ
            # worker çº¿ç¨‹æ£€æµ‹åˆ°ä¿¡å·åä¼šæ“ä½œ stream.stop()

            current_session_id += 1
            expected_seq_id = 0
            recive_seq_id = 0
            raw_audio_heap.clear()

            interrupt_event.clear()

        # é”å®šå½“å‰è¯·æ±‚çš„ Session ID
        this_sid = current_session_id
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ç»™è¿™ä¸€æ•´æ®µæ–‡æœ¬åˆ†é…ä¸€ä¸ªèµ·å§‹åºå·
        # å¦‚æœä¸€æ®µè¯ä¼šäº§ç”Ÿå¤šä¸ªéŸ³é¢‘å—ï¼Œæˆ‘ä»¬éœ€è¦è®©å®ƒä»¬è¿ç»­
        start_seq = recive_seq_id

    # 2. æ¨ç†è¿‡ç¨‹ (ç›´æ¥å†™åœ¨æ¥å£å‡½æ•°é‡Œï¼Œä¸åŠ é”ï¼Œå¦åˆ™æ’­æ”¾çº¿ç¨‹ä¼šå¡æ­»)
    try:
        # è®°å½•å†…éƒ¨äº§ç”Ÿçš„å°å—åºå·
        internal_seq = start_seq
        generator = pipeline(text, voice=VOICE_NAME, speed=1.0)

        for _, _, audio in generator:
            # ã€å…³é”®ã€‘æ£€æŸ¥åœ¨è¿™ä¸ªå¾ªç¯è¿‡ç¨‹ä¸­ï¼Œæ˜¯å¦æœ‰æ–°è¯·æ±‚è¿›æ¥æŠŠ session åˆ·æ‰äº†
            if interrupt_event.is_set() or this_sid != current_session_id:
                print(f"ğŸš« æ­£åœ¨æ¨ç†æ—¶è¢«ä¸­æ­¢: sid={this_sid}")
                return {"status": "interrupted"}

            # å¤„ç†éŸ³é¢‘
            wav_data = audio.numpy() if hasattr(audio, 'numpy') else audio
            resampled = resample_poly(wav_data, up=int(TARGET_SR), down=int(KOKORO_OFFICIAL_SR)).astype(np.float32)

            # ã€å…³é”®ã€‘åªåœ¨æ¨å…¥å †çš„ä¸€ç¬é—´åŠ é”
            with GLOBAL_LOCK:
                if this_sid == current_session_id:
                    heapq.heappush(raw_audio_heap, (this_sid, internal_seq, resampled))
                    print(f"ğŸ“¦ å·²å…¥å †: sid={this_sid}, seq={internal_seq}")
                    internal_seq += 1

        # æ¨ç†å®Œåæ›´æ–°å…¨å±€æ¥æ”¶åºå·ï¼Œä¾›ä¸‹ä¸€ä¸ªæ–‡æœ¬ç‰‡æ®µä½¿ç”¨
        with GLOBAL_LOCK:
            if this_sid == current_session_id:
                recive_seq_id = internal_seq

    except Exception as e:
        print(f"æ¨ç†é”™è¯¯: {e}")
        return {"status": "error", "msg": str(e)}

    return {"status": "ok", "last_seq": internal_seq - 1}

if __name__ == "__main__":
    generator = pipeline("å‘éŸ³æœåŠ¡å·²å¯åŠ¨", voice=VOICE_NAME, speed=1.0)
    for _, _, audio in generator:
        wav = audio.numpy()
        if isinstance(wav, torch.Tensor):
            wav_data = wav.view(-1).cpu().numpy()
        else:
            wav_data =wav
        data_resampled = resample_poly(wav_data, up=44100, down=KOKORO_OFFICIAL_SR)
        # print("â–¶ï¸ æ’­æ”¾éŸ³é¢‘...")
        sd.play(data_resampled, samplerate=44100,device=output_sound_index)
    uvicorn.run(app, host="127.0.0.1", port=28185, log_level="warning")