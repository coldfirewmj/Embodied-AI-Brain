import os
import sys
import time
import threading
import heapq
import numpy as np
import torch
import sounddevice as sd
from scipy.signal import resample_poly
import uvicorn
from fastapi import FastAPI, Request

# ===== é…ç½® =====
KOKORO_OFFICIAL_SR = 24000.0
TARGET_SR = 44100
VOICE_NAME = "zm_yunxi"

# åŠ è½½æ¨¡å‹ (ä»£ç ä¿æŒä¸å˜)
original_cwd = os.getcwd()
sys.path.append(os.path.join(original_cwd, 'audio', 'kokoro'))
from kokoro import KPipeline, KModel

model = KModel(config=original_cwd + "/Models/Kokoro-82M/config.json",
              model=original_cwd + "/Models/Kokoro-82M/kokoro-v1_0.pth",
              disable_complex=True)
pipeline = KPipeline(lang_code='z', device="cuda", model=model)

# ===== å˜é‡æ§åˆ¶ =====
raw_audio_heap = [] 
interrupt_event = threading.Event()
# last_request_timeè®°å½•ä¸Šæ¬¡è¯·æ±‚æ—¶é—´
last_request_time = 0
# current_session_idè¡¨ç¤ºæ¯ä¸€è½®å¯¹è¯çš„è½®æ•°
current_session_id = 0 
# global_seq_idç”¨äºä¸ºæ¯ä¸ªè¯·æ±‚åˆ†é…å”¯ä¸€çš„åºå·
global_seq_id = 0    
expected_seq_id = 0  
LOCK = threading.Lock()

# æŸ¥æ‰¾è®¾å¤‡
def get_output_device_index(target_name='default'):
    devices = sd.query_devices()
    for idx, dev in enumerate(devices):
        if target_name in dev['name'] and dev['max_output_channels'] > 0:
            return idx
    return None
output_sound_index = get_output_device_index()

# ===== 1. æ’­æ”¾æ¶ˆè´¹è€…çº¿ç¨‹ (ä¿®æ­£é€»è¾‘) =====
def tts_playback_worker():
    global expected_seq_id, raw_audio_heap, current_session_id
    print("ğŸš€ æ’­æ”¾çº¿ç¨‹å·²å°±ç»ªï¼Œç›‘å¬ 28185...")
    
    while True:
        audio_to_play = None
        
        with LOCK:
            if raw_audio_heap:
                # å †é¡¶æ•°æ®: (sid, seq_id, audio_data)
                # heapq åªæ¯”è¾ƒå‰ä¸¤ä¸ªå…ƒç´ ï¼Œç›´åˆ°ç¡®å®šå”¯ä¸€æ€§
                top_sid, top_seq_id, _ = raw_audio_heap[0]
                
                # æƒ…å†µ A: æ—§ Session çš„åƒåœ¾æ•°æ®ï¼Œç›´æ¥æ¸…é™¤
                if top_sid < current_session_id:
                    heapq.heappop(raw_audio_heap)
                    continue
                
                # æƒ…å†µ B: è½®åˆ°å½“å‰åºå·æ’­æ”¾
                if top_sid == current_session_id and top_seq_id == expected_seq_id:
                    _, _, audio_to_play = heapq.heappop(raw_audio_heap)
                    # print(f"â–¶ï¸ æå–æˆåŠŸ: Session {top_sid}, Seq {top_seq_id}")
        
        if audio_to_play is not None:
            print(f"ğŸ”Š æ­£åœ¨æ’­æ”¾ Seq: {expected_seq_id}")
            sd.play(audio_to_play, samplerate=TARGET_SR, device=output_sound_index)
            
            duration = len(audio_to_play) / TARGET_SR
            start_t = time.time()
            while time.time() - start_t < duration:
                if interrupt_event.is_set():
                    sd.stop()
                    break
                time.sleep(0.01)
            
            with LOCK:
                expected_seq_id += 1
        else:
            # æ²¡è½®åˆ°æˆ–æ²¡æ•°æ®ï¼ŒçŸ­ä¼‘çœ 
            time.sleep(0.02)

threading.Thread(target=tts_playback_worker, daemon=True).start()

# ===== 2. æ¨ç†ç”Ÿäº§è€…ä»»åŠ¡ =====
def inference_task(text, sid, seq_id):
    try:
        # print(f"âš™ï¸ æ¨ç†å¼€å§‹: [ID {seq_id}] {text[:10]}...")
        generator = pipeline(text, voice=VOICE_NAME, speed=1.0)
        
        for _, _, audio in generator:
            if sid != current_session_id or interrupt_event.is_set():
                return
            
            # é¢„å¤„ç†æ•°æ®
            wav = audio.numpy() if hasattr(audio, 'numpy') else audio
            if isinstance(wav, torch.Tensor):
                wav_data = wav.view(-1).cpu().numpy()
            else:
                wav_data = wav
            
            resampled = resample_poly(wav_data, up=int(TARGET_SR), down=int(KOKORO_OFFICIAL_SR)).astype(np.float32)
            
            # ã€å…³é”®ä¿®å¤ã€‘: å­˜å…¥å †ã€‚
            # ä¸ºäº†é˜²æ­¢ heapq æ¯”è¾ƒ NumPy æ•°ç»„ï¼Œæˆ‘ä»¬å°†æ•°æ®æ”¾åœ¨åˆ—è¡¨çš„ç¬¬ä¸‰ä½
            # Python çš„æ¯”è¾ƒè§„åˆ™æ˜¯ï¼šå…ˆæ¯” sid, å†æ¯” seq_id, åªè¦ seq_id ä¸åŒå°±ä¸å†å¾€åæ¯”ã€‚
            with LOCK:
                if sid == current_session_id:
                    heapq.heappush(raw_audio_heap, (sid, seq_id, resampled))
                    # print(f"âœ… æ¨ç†å®Œæˆå…¥å †: Seq {seq_id}")
                    
    except Exception as e:
        print(f"âŒ æ¨ç†å¼‚å¸¸: {e}")

# ===== 3. FastAPI æ¥å£ =====
app = FastAPI()

@app.post("/v1")
async def receive_tts_text(request: Request):
    global last_request_time, interrupt_event, current_session_id, global_seq_id, expected_seq_id, raw_audio_heap
    
    try:
        body = await request.json()
        text = body.get("text", "").strip()
        if not text: return {"status": "empty"}

        current_time = time.time()
        
        with LOCK:
            time_diff = current_time - last_request_time
            
            # 5ç§’æ‰“æ–­é€»è¾‘
            if time_diff > 5.0:
                print(f"\nâš¡ æ‰“æ–­å¹¶é‡ç½®å¯¹è¯ (é—´éš” {time_diff:.1f}s)")
                interrupt_event.set()
                sd.stop()
                
                current_session_id += 1 
                global_seq_id = 0      
                expected_seq_id = 0    
                raw_audio_heap = []    
                
                time.sleep(0.05) 
                interrupt_event.clear()
            
            target_sid = current_session_id
            target_seq = global_seq_id
            global_seq_id += 1
            last_request_time = current_time

            # å¯åŠ¨æ¨ç†
            threading.Thread(target=inference_task, args=(text, target_sid, target_seq), daemon=True).start()
            
        return {"status": "ok", "seq": target_seq}

    except Exception as e:
        return {"status": "error", "msg": str(e)}

if __name__ == "__main__":
    generator = pipeline("å‘éŸ³æœåŠ¡å·²å¯åŠ¨", voice=VOICE_NAME, speed=1.0)
    for _, _, audio in generator:
        wav = audio.numpy()
        if isinstance(wav, torch.Tensor):
            wav_data = wav.view(-1).cpu().numpy()
        else:
            wav_data =wav
        data_resampled = resample_poly(wav_data, up=44100, down=KOKORO_OFFICIAL_SR)
        # print("â–¶ï¸ æ’­æ”¾éŸ³é¢‘...")
        sd.play(data_resampled, samplerate=44100)
    uvicorn.run(app, host="0.0.0.0", port=28185, log_level="warning")