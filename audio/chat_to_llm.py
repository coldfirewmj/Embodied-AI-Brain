import os
import sys
import time
import numpy as np
from textwrap import dedent
import sounddevice as sd
import sherpa_onnx
print(sherpa_onnx.__file__)


model_dir = "/home/aiot/mingjuwang/Models/kokoro-multi-lang-v1_1"
start_time = time.time()
config = sherpa_onnx.OfflineTtsConfig(
    model=sherpa_onnx.OfflineTtsModelConfig(
        kokoro=sherpa_onnx.OfflineTtsKokoroModelConfig(
            model=os.path.join(model_dir,"model.onnx"),
            voices=os.path.join(model_dir,"voices.bin"),
            tokens=os.path.join(model_dir,"tokens.txt"),
            data_dir=os.path.join(model_dir,"espeak-ng-data"),
            dict_dir=os.path.join(model_dir,"dict"),
            lexicon=model_dir+"/lexicon-zh.txt,"+model_dir+"/lexicon-us-en.txt",
        ),
        num_threads=2,
        provider="cuda",  # æœ‰GPUåˆ™æ”¹ä¸º"cuda"ï¼ˆéœ€å®‰è£…onnxruntime-gpuï¼‰
    ),
    rule_fsts=model_dir+"/phone-zh.fst,"+
            model_dir+"/date-zh.fst,"+
            model_dir+"/number-zh.fst",
)
# 2. åˆ›å»ºTTSå¼•æ“
tts = sherpa_onnx.OfflineTts(config)
print(f"ğŸ‰ åŠ è½½kokoroæ¨¡å‹è€—æ—¶: {time.time() - start_time:.2f} ç§’")

original_cwd = os.getcwd()
sys.path.append(original_cwd+'/RealTimeSTT')
from RealtimeSTT import AudioToTextRecorder
def get_in_sounddevice_index(target_name='ReSpeaker'):
    devices = sd.query_devices()
    for idx, device in enumerate(devices):
        if target_name in device['name'] and device['max_input_channels'] > 0:
            print("éº¦å…‹é£:",device['name'],' index:',idx)
            return idx
    return None
input_sound_index = get_in_sounddevice_index()
# 1. ä¿å­˜å½“å‰å·¥ä½œç›®å½•
LOCAL_MODEL_PATH = os.path.dirname(original_cwd)+"/Models/faster-whisper-large-v3-turbo"
# ç¡®ä¿è·¯å¾„å­˜åœ¨
# åˆ›å»ºå½•éŸ³+è¯†åˆ«å™¨ï¼ˆå…³é”®é…ç½®ï¼‰
start_time = time.time()
recorder = AudioToTextRecorder(
    # æ¨¡å‹å¤§å°ï¼š/base
    model=LOCAL_MODEL_PATH,    
    silero_vad_path= os.path.dirname(original_cwd)+'/Models/snakers4_silero-vad_master',
    # å¼ºåˆ¶ä¸­æ–‡ï¼ˆæé«˜å‡†ç¡®ç‡ï¼‰       
    language="zh",         
    compute_type="float16",   
    device="cuda",  
    # æ˜¯å¦ä½¿ç”¨éº¦å…‹é£è¾“å…¥ï¼ŒFalseè¡¨ç¤ºä½¿ç”¨å›è°ƒå‡½æ•°è¾“å…¥
    use_microphone=True, 
    # æ·»åŠ å”¤é†’è¯
    # wake_words=WAKE_WORD,
    # wake_words_sensitivity=SENSITIVITY,       
    initial_prompt="ä»¥ä¸‹æ˜¯æ™®é€šè¯çš„å¥å­ã€‚",
    # å¦‚æœä½ çŸ¥é“éº¦å…‹é£è®¾å¤‡ç´¢å¼•ï¼Œå–æ¶ˆæ³¨é‡Šä¸‹ä¸€è¡Œï¼š
    input_device_index=input_sound_index,  # æ›¿æ¢ä¸ºä½ çš„éº¦å…‹é£ç´¢å¼•ï¼ˆé€šè¿‡ sounddevice_devices.py è·å–ï¼‰
)
print(f"ğŸ‰ åŠ è½½whisperæ¨¡å‹è€—æ—¶: {time.time() - start_time:.2f} ç§’")
recorder.stop()
start_time= time.time()

import re
def preprocess_voice_text(text):
    if not text:
        return None
    text = text.strip()
    if not text or text.isspace():
        return None
    text = re.sub(r"^(å—¯|å•Š|å“¦|å‘ƒ|å“)\s*", "", text)
    text = text[:2000]
    print(text)
    return text

def get_out_ounddevice_index(target_name='USB2.0 Device'):
    devices = sd.query_devices()
    for idx, device in enumerate(devices):
        if target_name in device['name'] and device['max_input_channels'] > 0:
            print("æ‰¬å£°å™¨:",device['name'],' index:',idx)
            return idx
    return None
output_device_index = get_out_ounddevice_index()

def play_audio(audio, gain=5.0):
    samples = np.array(audio.samples, dtype=np.float32) * gain
    samples = np.clip(samples, -1.0, 1.0)
    from scipy.signal import resample
    # é‡é‡‡æ ·åˆ° 48000 Hz
    if audio.sample_rate != 48000:
        num_samples = int(len(samples) * 48000 / audio.sample_rate)
        samples = resample(samples, num_samples)
    sd.play(samples, samplerate=48000, device=output_device_index)
    sd.wait()

from openai import OpenAI
client = OpenAI(base_url="http://localhost:8000/v1", api_key="EMPTY")
sentence_endings = r'[ã€‚ï¼ï¼Ÿ!?ï¼›;â€¦\n]'  # ä¸­è‹±æ–‡å¥å°¾ç¬¦å·


def main():
    while True:
        recorder.start()
        text = preprocess_voice_text(recorder.text())
        recorder.stop()
        messages = [{"role": "user", "content": text}]
        response = client.chat.completions.create(
            model='Qwen2.5-VL-7B-Instruct',
            messages=messages,
            temperature=0.2,
            extra_body={"mm_processor_kwargs":{"fps": [1]}},
            stream=True,
        )
        buffer = ""
        for chunk in response:
            if chunk.choices and chunk.choices[0].delta.content:
                token = chunk.choices[0].delta.content
                buffer += token
                print(token, end="", flush=True)  # ç»ˆç«¯ä¹Ÿæ˜¾ç¤º

                # æ£€æŸ¥æ˜¯å¦é‡åˆ°å®Œæ•´å¥å­ç»“å°¾
                if re.search(sentence_endings, token):
                    # å»æ‰æœ«å°¾ç©ºç™½
                    sentence = buffer.strip()
                    if sentence:
                        print("\nğŸ”Š æ­£åœ¨æœ—è¯»æ­¤å¥...")
                        try:
                            audio = tts.generate(sentence, sid=10, speed=1.0)
                            print("=====================================")
                            if len(audio.samples) > 0:
                                print("+++++++++++++++++++++++++++++++++++++++")
                                play_audio(audio)
                        except Exception as e:
                            print(f"\nâš ï¸ TTS é”™è¯¯: {e}")
                        buffer = ""  # æ¸…ç©ºç¼“å†²åŒº

        # å¤„ç†æœ€åä¸€å¥ï¼ˆå¦‚æœæ²¡ä»¥å¥å·ç»“å°¾ï¼‰
        if buffer.strip():
            sentence = buffer.strip()
            print(f"\nğŸ”Š æœ—è¯»æœ€åä¸€å¥: {sentence}")
            try:
                audio = tts.generate(sentence, sid=10, speed=1.0)
                if len(audio.samples) > 0:
                    play_audio(audio)
            except Exception as e:
                print(f"\nâš ï¸ TTS é”™è¯¯: {e}")

if __name__ == "__main__": 
    voice_control = True
    main()