import os
import sys
original_cwd = os.getcwd()
sys.path.append(original_cwd+'/RealTimeSTT')
from RealtimeSTT import AudioToTextRecorder
import time
from textwrap import dedent
os.environ["JACK_NO_START_SERVER"] = "1"
os.environ["PYAUDIO_ALSA_ERROR_QUIET"] = "1"
# -------------------------- æ”¹æ­¥éª¤ä¸ºäº†ä½¿create_vlm_openaiæ­£å¸¸åŠ è½½json --------------------------
# 1. ä¿å­˜å½“å‰å·¥ä½œç›®å½•
LOCAL_MODEL_PATH = original_cwd+"/faster-whisper-large-v3-turbo"
# åˆ›å»ºå½•éŸ³+è¯†åˆ«å™¨ï¼ˆå…³é”®é…ç½®ï¼‰
start_time = time.time()
recorder = AudioToTextRecorder(
    # æ¨¡å‹å¤§å°ï¼š/base
    model=LOCAL_MODEL_PATH,    
    # å¼ºåˆ¶ä¸­æ–‡ï¼ˆæé«˜å‡†ç¡®ç‡ï¼‰       
    language="zh",         
    compute_type="float16",   
    device="cuda",          
    # å¯è‡ªå®šä¹‰å›è°ƒ 
    # on_recording_start=lambda: None,   
    # on_recording_stop=lambda: None,
    # on_transcription_start=lambda: None,
    initial_prompt="ä»¥ä¸‹æ˜¯æ™®é€šè¯çš„å¥å­ã€‚",
    # å¦‚æœä½ çŸ¥é“éº¦å…‹é£è®¾å¤‡ç´¢å¼•ï¼Œå–æ¶ˆæ³¨é‡Šä¸‹ä¸€è¡Œï¼š
    input_device_index=38,  # æ›¿æ¢ä¸ºä½ çš„éº¦å…‹é£ç´¢å¼•ï¼ˆé€šè¿‡ sounddevice_devices.py è·å–ï¼‰
)
print(f"ğŸ‰ åŠ è½½æ¨¡å‹è€—æ—¶: {time.time() - start_time:.2f} ç§’")
# 2. åˆ‡æ¢åˆ° rabbitbot é¡¹ç›®æ ¹ç›®å½•
project_root = "/home/aiot/fuchengjia/Projects/rabbitbot-dev-ros2"
os.chdir(project_root)
sys.path.insert(0, project_root)
from rabbitbot.provider import create_vlm_openai
print("åŠ è½½å¤§æ¨¡å‹æˆåŠŸ")
# 3. åˆ‡æ¢å›æ¥åŸæ¥çš„ç¯å¢ƒ
# os.chdir(original_cwd)

def preprocess_voice_text(text):
    if not text:
        return None
    text = text.strip()
    if not text or text.isspace():
        return None
    import re
    text = re.sub(r"^(å—¯|å•Š|å“¦|å‘ƒ|å“)\s*", "", text)
    text = text[:2000]
    return text

def main():
    print("ğŸ™ï¸ åˆå§‹åŒ– RealtimeSTTï¼ˆä½¿ç”¨æœ¬åœ° faster-whisperï¼‰...")
    vlm_openai = create_vlm_openai()
    vlm_openai.prompt = dedent(f"""\
            ä½ æ˜¯ä¸€åæ¥çº¿å‘˜ï¼Œä»…æ ¹æ®ç”¨æˆ·**è¾“å…¥æ–‡æœ¬**çš„ä½ç½®ã€‚
            è¯·è¿›è¡Œå¯¹åº”çš„å›å¤ï¼š

            **è¾“å‡ºè¦æ±‚ï¼š**
            è¯·ç›´æ¥é’ˆå¯¹ç”¨æˆ·çš„æ–‡å­—å›å¤ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡å­—ã€‚
            """)


    print("âœ… å‡†å¤‡å°±ç»ªï¼è¯·å¯¹ç€éº¦å…‹é£è¯´è¯ï¼ˆä¸­æ–‡ï¼‰...")
    print("-" * 60)

    try:
        while True:
            # è·å–å½“å‰æœ€æ–°è½¬å½•æ–‡æœ¬ï¼ˆéé˜»å¡ï¼‰
            text = preprocess_voice_text(recorder.text())
            if text:
                print(f"ğŸ—£ï¸ è¯†åˆ«ç»“æœ: {text}")
                messages = [{"role": "user", "content": text}]
                for _ in range(1):
                    json_content = vlm_openai.get_chat_response(
                        messages=messages,
                        extra_body={})
                # æ¸…ç©ºå·²è¯»æ–‡æœ¬ï¼Œé¿å…é‡å¤è¾“å‡º
                print(f"ğŸ¤– å›å¤å†…å®¹: {json_content}")
                recorder.text("")

    except KeyboardInterrupt:
        print("\nğŸ‘‹ é€€å‡ºç¨‹åº")
    finally:
        recorder.shutdown()  # é‡Šæ”¾èµ„æº
        
if __name__ == "__main__":
    main()
