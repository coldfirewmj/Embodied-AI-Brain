import sherpa_onnx
print(sherpa_onnx.__file__)
import sounddevice as sd
import numpy as np
import os
import wave
import time
import soundfile as sf

model_dir = "/home/aiot/mingjuwang/Models/kokoro-multi-lang-v1_1"
def get_sounddevice_index(target_name='USB2.0 Device'):
    devices = sd.query_devices()
    for idx, device in enumerate(devices):
        if target_name in device['name'] and device['max_output_channels'] > 0:
            return idx
    return None
output_device_index = get_sounddevice_index()
text = "Helloï¼Œæ¬¢è¿ä½¿ç”¨ kokoro-multi-lang-v1_1 æ¨¡å‹ï¼Œå½“å‰æ¸©åº¦ 25â„ƒï¼Œè‹±æ–‡æµ‹è¯•ï¼šThis is a test."
rule_fsts = [
    os.path.join(model_dir, "phone-zh.fst"),
    os.path.join(model_dir, "date-zh.fst"),
    os.path.join(model_dir, "number-zh.fst"),
]
config = sherpa_onnx.OfflineTtsConfig(
    model=sherpa_onnx.OfflineTtsModelConfig(
        kokoro=sherpa_onnx.OfflineTtsKokoroModelConfig(
            model=os.path.join(model_dir,"model.onnx"),
            voices=os.path.join(model_dir,"voices.bin"),
            tokens=os.path.join(model_dir,"tokens.txt"),
            data_dir=os.path.join(model_dir,"espeak-ng-data"),
            dict_dir=os.path.join(model_dir,"dict"),
            lexicon=model_dir+"/lexicon-zh.txt,"+model_dir+"/lexicon-us-en.txt",
        ),
        num_threads=2,
        provider="cuda",  # æœ‰GPUåˆ™æ”¹ä¸º"cuda"ï¼ˆéœ€å®‰è£…onnxruntime-gpuï¼‰
    ),
    rule_fsts=model_dir+"/phone-zh.fst,"+
            model_dir+"/date-zh.fst,"+
            model_dir+"/number-zh.fst",
)

# 2. åˆ›å»ºTTSå¼•æ“
tts = sherpa_onnx.OfflineTts(config)
import re
from openai import OpenAI
client = OpenAI(base_url="http://localhost:8000/v1", api_key="EMPTY")
messages = [{"role": "user", "content": "è¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½çš„å‘å±•å†å²ã€‚"}]
sentence_endings = r'[ã€‚ï¼ï¼Ÿ!?ï¼›;â€¦\n]'  # ä¸­è‹±æ–‡å¥å°¾ç¬¦å·
def play_audio(audio, gain=5.0):
    samples = np.array(audio.samples, dtype=np.float32) * gain
    samples = np.clip(samples, -1.0, 1.0)
    from scipy.signal import resample
    # é‡é‡‡æ ·åˆ° 48000 Hz
    if audio.sample_rate != 48000:
        num_samples = int(len(samples) * 48000 / audio.sample_rate)
        samples = resample(samples, num_samples)
    sd.play(samples, samplerate=48000, device=output_device_index)
    sd.wait()
def main():
    response = client.chat.completions.create(
        model='Qwen2.5-VL-7B-Instruct',
        messages=messages,
        temperature=0.2,
        extra_body={"mm_processor_kwargs":{"fps": [1]}},
        stream=True,
    )
    # å°è¯•æ”¹å˜ sid çœ‹çœ‹æ˜¯å¦æœ‰å˜åŒ–
    # audio = tts.generate(text, sid=1, speed=1.0)
    buffer = ""
    for chunk in response:
        if chunk.choices and chunk.choices[0].delta.content:
            token = chunk.choices[0].delta.content
            buffer += token
            print(token, end="", flush=True)  # ç»ˆç«¯ä¹Ÿæ˜¾ç¤º

            # æ£€æŸ¥æ˜¯å¦é‡åˆ°å®Œæ•´å¥å­ç»“å°¾
            if re.search(sentence_endings, token):
                # å»æ‰æœ«å°¾ç©ºç™½
                sentence = buffer.strip()
                if sentence:
                    print("\nğŸ”Š æ­£åœ¨æœ—è¯»æ­¤å¥...")
                    try:
                        audio = tts.generate(sentence, sid=1, speed=1.0)
                        print("=====================================")
                        if len(audio.samples) > 0:
                            print("+++++++++++++++++++++++++++++++++++++++")
                            play_audio(audio)
                    except Exception as e:
                        print(f"\nâš ï¸ TTS é”™è¯¯: {e}")
                    buffer = ""  # æ¸…ç©ºç¼“å†²åŒº

    # å¤„ç†æœ€åä¸€å¥ï¼ˆå¦‚æœæ²¡ä»¥å¥å·ç»“å°¾ï¼‰
    if buffer.strip():
        sentence = buffer.strip()
        print(f"\nğŸ”Š æœ—è¯»æœ€åä¸€å¥: {sentence}")
        try:
            audio = tts.generate(sentence, sid=1, speed=1.0)
            if len(audio.samples) > 0:
                play_audio(audio)
        except Exception as e:
            print(f"\nâš ï¸ TTS é”™è¯¯: {e}")
    print('è¯­éŸ³ç”Ÿæˆè€—æ—¶: {:.2f} ç§’'.format(time.time() - start_time))
    print("Audio type:", type(audio))
    print("Audio :", audio)

    if audio and len(audio.samples) > 0:
        samples = np.array(audio.samples).flatten()
        actual_duration = len(samples) / audio.sample_rate
        print(f"å®é™…ç”ŸæˆéŸ³é¢‘é•¿åº¦: {actual_duration:.2f} ç§’")
        
        # 3. æ’­æ”¾
        # print("æ’­æ”¾ä¸­...")
        # sd.play(np.array(audio.samples)*8.0, samplerate=audio.sample_rate, device=output_device_index)
        # sd.wait() # æ ¸å¿ƒï¼šå¿…é¡»ç­‰å¾…æ’­æ”¾å®Œæˆ

        # 4. ä¿å­˜
        # with wave.open("debug_output.wav", "wb") as wf:
        #     wf.setnchannels(1)
        #     wf.setsampwidth(2)
        #     wf.setframerate(audio.sample_rate)
        #     data = np.clip(samples * 32767, -32768, 32767).astype(np.int16)
        #     wf.writeframes(data.tobytes())
    else:
        print("ç”Ÿæˆå¤±è´¥ã€‚")

if __name__ == "__main__":
    main()