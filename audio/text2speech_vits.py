import sherpa_onnx
print(sherpa_onnx.__file__)
import sounddevice as sd
import numpy as np
import os
import wave
import time

# è¯·ç¡®ä¿æ­¤è·¯å¾„ä¸‹æ˜¯æ¨¡å‹ä½ç½®
MODEL_BASE_DIR = "/home/aiot/mingjuwang/Models/vits-zh-aishell3"
def get_sounddevice_index(target_name='USB2.0 Device'):
    devices = sd.query_devices()
    for idx, device in enumerate(devices):
        if target_name in device['name'] and device['max_input_channels'] > 0:
            return idx
    return None
output_device_index = get_sounddevice_index()
# print("ä½¿ç”¨çš„è¾“å‡ºè®¾å¤‡ç´¢å¼•:", output_device_index)
# ç¼“å†²åŒº
sentence_endings = r'[ã€‚ï¼ï¼Ÿ!?ï¼›;â€¦\n]'  # ä¸­è‹±æ–‡å¥å°¾ç¬¦å·
def play_audio(audio, gain=5.0):
    samples = np.array(audio.samples, dtype=np.float32) * gain
    samples = np.clip(samples, -1.0, 1.0)
    from scipy.signal import resample
    # é‡é‡‡æ ·åˆ° 48000 Hz
    if audio.sample_rate != 48000:
        num_samples = int(len(samples) * 48000 / audio.sample_rate)
        samples = resample(samples, num_samples)
    sd.play(samples, samplerate=48000, device=output_device_index)
    sd.wait()
import re
from openai import OpenAI
client = OpenAI(base_url="http://localhost:8000/v1", api_key="EMPTY")
messages = [{"role": "user", "content": "è¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½çš„å‘å±•å†å²ã€‚"}]
def run_sherpa_tts():
    start_time = time.time()
    # 1. é…ç½®
    vits_model_config = sherpa_onnx.OfflineTtsVitsModelConfig(
        model=os.path.join(MODEL_BASE_DIR, "vits-aishell3.onnx"),
        lexicon=os.path.join(MODEL_BASE_DIR, "lexicon.txt"),
        tokens=os.path.join(MODEL_BASE_DIR, "tokens.txt"),
        data_dir="", # ä½¿ç”¨å®˜æ–¹ä¸‹è½½çš„è·¯å¾„
        noise_scale=0.667,
        noise_scale_w=0.8,
        length_scale=1.0  # ç¨å¾®è°ƒæ…¢ä¸€ç‚¹ï¼Œä¾¿äºå¬æ¸…
    )

    config = sherpa_onnx.OfflineTtsConfig(
        model=sherpa_onnx.OfflineTtsModelConfig(
            vits=vits_model_config,
            # num_threads=14,
            debug=False,
            provider="cuda"
        )
    )
    
    tts = sherpa_onnx.OfflineTts(config)
    print('é…ç½®æ¨¡å‹è€—æ—¶: {:.2f} ç§’'.format(time.time() - start_time))
    start_time = time.time()
    # 2. ç”Ÿæˆ
    text = "åºŠå‰æ˜æœˆå…‰ï¼Œç–‘æ˜¯åœ°ä¸Šéœœã€‚"
    # print(f"æ­£åœ¨ç”Ÿæˆ: {text}")
    response = client.chat.completions.create(
        model='Qwen2.5-VL-7B-Instruct',
        messages=messages,
        temperature=0.2,
        extra_body={"mm_processor_kwargs":{"fps": [1]}},
        stream=True,
    )
    # å°è¯•æ”¹å˜ sid çœ‹çœ‹æ˜¯å¦æœ‰å˜åŒ–
    # audio = tts.generate(text, sid=1, speed=1.0)
    buffer = ""
    for chunk in response:
        if chunk.choices and chunk.choices[0].delta.content:
            token = chunk.choices[0].delta.content
            buffer += token
            print(token, end="", flush=True)  # ç»ˆç«¯ä¹Ÿæ˜¾ç¤º

            # æ£€æŸ¥æ˜¯å¦é‡åˆ°å®Œæ•´å¥å­ç»“å°¾
            if re.search(sentence_endings, token):
                # å»æ‰æœ«å°¾ç©ºç™½
                sentence = buffer.strip()
                if sentence:
                    print("\nğŸ”Š æ­£åœ¨æœ—è¯»æ­¤å¥...")
                    try:
                        audio = tts.generate(sentence, sid=94, speed=1.0)
                        print("=====================================")
                        if len(audio.samples) > 0:
                            print("+++++++++++++++++++++++++++++++++++++++")
                            play_audio(audio)
                    except Exception as e:
                        print(f"\nâš ï¸ TTS é”™è¯¯: {e}")
                    buffer = ""  # æ¸…ç©ºç¼“å†²åŒº

    # å¤„ç†æœ€åä¸€å¥ï¼ˆå¦‚æœæ²¡ä»¥å¥å·ç»“å°¾ï¼‰
    if buffer.strip():
        sentence = buffer.strip()
        print(f"\nğŸ”Š æœ—è¯»æœ€åä¸€å¥: {sentence}")
        try:
            audio = tts.generate(sentence, sid=94, speed=1.0)
            if len(audio.samples) > 0:
                play_audio(audio)
        except Exception as e:
            print(f"\nâš ï¸ TTS é”™è¯¯: {e}")
    print('è¯­éŸ³ç”Ÿæˆè€—æ—¶: {:.2f} ç§’'.format(time.time() - start_time))
    print("Audio type:", type(audio))
    print("Audio :", audio)

    if audio and len(audio.samples) > 0:
        samples = np.array(audio.samples).flatten()
        actual_duration = len(samples) / audio.sample_rate
        print(f"å®é™…ç”ŸæˆéŸ³é¢‘é•¿åº¦: {actual_duration:.2f} ç§’")
        
        # 3. æ’­æ”¾
        # print("æ’­æ”¾ä¸­...")
        # sd.play(np.array(audio.samples)*8.0, samplerate=audio.sample_rate, device=output_device_index)
        # sd.wait() # æ ¸å¿ƒï¼šå¿…é¡»ç­‰å¾…æ’­æ”¾å®Œæˆ

        # 4. ä¿å­˜
        # with wave.open("debug_output.wav", "wb") as wf:
        #     wf.setnchannels(1)
        #     wf.setsampwidth(2)
        #     wf.setframerate(audio.sample_rate)
        #     data = np.clip(samples * 32767, -32768, 32767).astype(np.int16)
        #     wf.writeframes(data.tobytes())
    else:
        print("ç”Ÿæˆå¤±è´¥ã€‚")

if __name__ == "__main__":
    run_sherpa_tts()