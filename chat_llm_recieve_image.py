import os
import sys
original_cwd = os.getcwd()
import time
from textwrap import dedent
import requests
from urllib.parse import urljoin
import json
from prompt_build import get_inst_plan,get_inst_chat,get_inst_find

import threading
import uvicorn
from fastapi import FastAPI, Request
import queue
import cv2
# çº¿ç¨‹å®‰å…¨é˜Ÿåˆ—ï¼ˆæ¨èï¼‰
stt_queue = queue.Queue(maxsize=5)
# æ¥æ”¶STTæœåŠ¡å™¨çš„æ¶ˆæ¯
app = FastAPI(title="Main STT Receiver")

@app.post("/v1")
async def receive_stt_text(request: Request):
    try:
        body = await request.json()  # â† å¿…é¡» awaitï¼
        text = body.get("text", "")
        if text and text.strip():
            stt_queue.put(text.strip())
            print(f"ğŸ“¨ ä¸»è¿›ç¨‹æ”¶åˆ°STT: {text}")
        return {"status": "ok"}
    except Exception as e:
        print(f"âŒ è§£æJSONå¤±è´¥: {e}")
        return {"status": "error", "message": str(e)}

def start_stt_receiver():
    """åœ¨åå°çº¿ç¨‹å¯åŠ¨HTTPæ¥æ”¶æœåŠ¡"""
    uvicorn.run(app, host="127.0.0.1", port=28184, log_level="warning")
    print("STT receiver started."," host:","127.0.0.1"," port:",28184)

# å¯åŠ¨æ¥æ”¶æœåŠ¡ï¼ˆdaemon=True ç¡®ä¿éšä¸»è¿›ç¨‹é€€å‡ºï¼‰
threading.Thread(target=start_stt_receiver, daemon=True).start()

# TTSå‘é€ç»™æœåŠ¡å™¨
class TTSAgent:
    def __init__(self, host_url):
        print(f"TTSAgent: host_url {host_url}")
        self.host_url = host_url

    def run(self, input_dict: dict) -> str:
        # ç›´æ¥ä¼ å…¥å­—å…¸ï¼Œä¸è¦åœ¨å¤–é¢è½¬å­—ç¬¦ä¸²
        try:
            # è¿™é‡Œçš„è·¯å¾„ç›´æ¥æŒ‡å‘ /v1ï¼Œä¸ä½¿ç”¨ 'exec'
            resp = requests.post(self.host_url, json=input_dict, timeout=5)
            if resp.status_code == 200:
                return "ok"
        except Exception as e:
            print(f"TTSAgent Error: {e}")
        return ""

tts = TTSAgent("http://127.0.0.1:28185/v1")

def tts_sound(tts_agent, text, lang, interrupt):
    # æ„é€ å­—å…¸
    input_dict = {"task": "text_to_speech", "lang": lang, "text": text, "interrupt": interrupt}
    print(f"ğŸ“¤ å‘é€ä¸­: {text}")

    # ç›´æ¥å‘é€
    tts_agent.run(input_dict)
    return 0

# åˆ‡æ¢åˆ°sam2å·¥ä½œç›®å½•ï¼ŒåŠ è½½sam
print('å¼€å§‹åŠ è½½sam2æ¨¡å‹')
start_time = time.time()
sam_path = original_cwd+'/camera/Grounded-SAM-2'
sys.path.append(sam_path)
from sam2.build_sam import build_sam2
from sam2.sam2_image_predictor import SAM2ImagePredictor
from PIL import Image
CHECKPOINT_SAM = os.path.join(sam_path,"sam2_checkpoints", "sam2.1_hiera_small.pt")
# CONFIG_SAMå‰é¢å¿…é¡»åŠ '/'
CONFIG_SAM = '/'+sam_path+"/sam2/configs/sam2.1/sam2.1_hiera_s.yaml"
# Load the model
sam2_model = build_sam2(CONFIG_SAM, CHECKPOINT_SAM, device="cuda")
sam_predictor = SAM2ImagePredictor(sam2_model)
print(f"ğŸ‰ åŠ è½½SAMæ¨¡å‹è€—æ—¶: {time.time() - start_time:.2f} ç§’")


# å¯åŠ¨å¤§æ¨¡å‹å®¢æˆ·ç«¯
print('å¼€å§‹è¿æ¥å¤§æ¨¡å‹æœåŠ¡')
start_time= time.time()
import base64
from openai import OpenAI
client = OpenAI(base_url="http://localhost:8000/v1", api_key="EMPTY")
print(f"ğŸ‰ åŠ è½½å¤§æ¨¡å‹æœåŠ¡è€—æ—¶: {time.time() - start_time:.2f} ç§’")
start_time= time.time()

import io
import numpy as np
# åˆæˆmessages
def input_messages_build(prompt, image=None):
    print(prompt)
    # æ„å»ºæ–‡æœ¬ä¿¡æ¯
    text_content = {
        "type": "text",
        "text": prompt
    }
    if image is None:
        return [
        {
            "role": "user",
            "content": [text_content],
        }]
    else:
        # 1. åˆ›å»ºä¸€ä¸ªå­—èŠ‚æµå®¹å™¨
        buffer = io.BytesIO()
        # 2. å°† PIL å›¾åƒä¿å­˜åˆ°å®¹å™¨ä¸­ï¼ˆæŒ‡å®šæ ¼å¼ï¼‰
        image.save(buffer, format="PNG")
        # è½¬æ¢ä¸º base64
        image_b64 = base64.b64encode(buffer).decode("utf-8")
        # æ„å»ºå›¾åƒä¿¡æ¯
        image_contents = {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/png;base64,{image_b64}"
                    },
                }
        return [
            {
                "role": "user",
                "content":   [text_content, image_contents],
            }]

import re
sentence_endings = r'[ã€‚ï¼ï¼Ÿ!?ï¼›;â€¦\n]'  # ä¸­è‹±æ–‡å¥å°¾ç¬¦å·
# å¾—åˆ°å›ç­”å¹¶è§£æå‡ºåŒ…å›´æ¡†
def get_respose_(model,text,image):
    start_time = time.time()
    # æ­¤å¤„åšä¸€ä¸ªåˆ¤æ–­ï¼Œå¦‚æœæ˜¯Cåˆ™èŠå¤©ï¼Œå¦‚æœæ˜¯Gåˆ™æŠ“å–ï¼Œç”±äºæ¥ä¸‹æ¥å¦‚æœæ˜¯Cåˆ™æµå¼è¾“å‡ºçš„ï¼Œæ‰€ä»¥éœ€è¦å…ˆåˆ¤æ–­
    
    prompt = get_inst_find(text)
    messages = input_messages_build(prompt, image)
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=0.2,
        extra_body={"mm_processor_kwargs":{"fps": [1]}},
        stream=False,
    )
    chunk = response.choices[0].message.content
    print("å›å¤ä¸ºï¼š",chunk)
    print("å›å¤ä¸ºï¼š",chunk)
    if chunk is None or chunk == '[]':
        tts_sound(tts,"æœªæ‰¾åˆ°ç‰©ä½“","zh",True)
        entity = []
        return entity
    entity = json.loads(chunk)
    print('åŒ…å›´æ¡†ä¸ºï¼š',entity)
    tts_sound(tts,"å·²æ‰¾åˆ°ç‰©ä½“","zh",True)

    # entity = stream_message.strip('```json\n').strip('```')
    # print("entityï¼š", entity)
    return entity


# å›¾åƒçº¿ç¨‹å®‰å…¨é˜Ÿåˆ—ï¼ˆæ¨èï¼‰
img_queue = queue.Queue(maxsize=1)
image_app = FastAPI(title="Image Receiver")
@image_app.post("/api/process_image")
async def receive_image(request: Request):
    # 1. è·å– JSON æ•°æ® (æ³¨æ„è¿™é‡Œå¿…é¡»ä½¿ç”¨ await)
    data = await request.json()
    image_data = data.get('image')

    # 2. å¤„ç† Base64 æ•°æ®å¤´
    if image_data.startswith('data:image'):
        image_data = image_data.split(',')[1]
    
    # 3. è§£ç  Base64 å›¾åƒ
    image_bytes = base64.b64decode(image_data)
    img = Image.open(io.BytesIO(image_bytes)).convert('RGB')
    img_queue.put(img)
    
    # åœ¨è¿™é‡Œè¿›è¡Œå›¾åƒå¤„ç†
    # mask = your_processing_function(img)
    
    # 4. è¿”å›å“åº”
    return {
        "success": True, 
        "message": "Image received successfully"
    }

def send_mask_to_gui(mask_image, gui_url="http://localhost:50052"):
    """
    å°† mask å‘é€åˆ° GUI
    
    Args:
        mask_image: PIL Image å¯¹è±¡ï¼ˆç°åº¦æ¨¡å¼ï¼ŒL modeï¼‰
        gui_url: GUI æœåŠ¡å™¨åœ°å€
    """
    # è½¬æ¢ä¸º Base64
    buffered = io.BytesIO()
    mask_image.save(buffered, format="PNG")
    mask_base64 = base64.b64encode(buffered.getvalue()).decode('utf-8')
    mask_data = f"data:image/png;base64,{mask_base64}"
    
    # å‘é€è¯·æ±‚
    response = requests.post(
        f"{gui_url}/external/receive_mask",
        headers={'Content-Type': 'application/json'},
        json={'mask_data': mask_data},
        timeout=10
    )
    
    if response.status_code == 200:
        result = response.json()
        if result.get('success'):
            print("Mask sent successfully")
        else:
            print(f"Failed: {result.get('message')}")
    else:
        print(f"HTTP Error: {response.status_code}")


def main():
    cv2.namedWindow("output image", cv2.WINDOW_AUTOSIZE)
    out_image = None
    while True:

        try:
            image = img_queue.get_nowait()
            out_image = image.copy()
            text = stt_queue.get_nowait()
            # è·å–åŒ…å›´æ¡†
            input_box = get_respose_('Qwen2.5-VL-7B-Instruct',text,image)
            if input_box!=[]:
                start_time = time.time()
                x1, y1, x2, y2 = input_box
                # -------------------------- SAM3æ¨ç† --------------------------
                inference_state = sam_predictor.set_image(image)
                masks, scores, _ = sam_predictor.predict(
                    box=input_box,
                    multimask_output=False  # å•maskæ›´é«˜æ•ˆ
                )
                print("æ¡†æ©ç é¢„æµ‹æ—¶é—´ä¸º",time.time()-start_time)
                print("å¯è§†åŒ–")
                mask = masks[np.argmax(scores)]
                # æ©ç è½¬äºŒå€¼å›¾
                mask_np = mask.astype(np.uint8) * 255
                # æ‰¾æ©ç è½®å»“
                contours, _ = cv2.findContours(mask_np, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                print("è½®å»“æ•°é‡ï¼š", len(contours))
                # ç»˜åˆ¶è½®å»“ï¼ˆçº¢è‰²ï¼Œçº¿å®½2ï¼‰
                cv2.drawContours(image, contours, -1, (0, 0, 255), 2)
                # ç»˜åˆ¶åŒ…å›´æ¡†ï¼ˆç»¿è‰²ï¼Œçº¿å®½2ï¼‰
                cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)
                cv2.namedWindow("detection window", cv2.WINDOW_AUTOSIZE)
                # å¼ºåˆ¶çª—å£ç½®é¡¶ï¼ˆæŸäº›Linuxæ¡Œé¢ç¯å¢ƒæ”¯æŒï¼‰
                cv2.setWindowProperty("detection window", cv2.WND_PROP_TOPMOST, 1)
                # -------------------------- å…³é”®æ­¥éª¤ï¼šæ˜¾ç¤ºå¹¶å¼ºåˆ¶åˆ·æ–° --------------------------
                cv2.imshow("detection window", image)
                # è¿™é‡Œçš„ waitKey éå¸¸å…³é”®ï¼Œå®ƒèƒ½è®©ç³»ç»Ÿæœ‰æ—¶é—´æ¸²æŸ“å‡ºè¿™ä¸ªæ–°çª—å£
                cv2.waitKey(1)
                # è·å¾—è¯¥ç‰©ä½“çš„xyzåæ ‡ï¼ˆåŒ…å›´æ¡†å†…æ©ç çš„åƒç´ ç‚¹å¯¹åº”çš„xyzæ±‚å¹³å‡ï¼‰
                # for u in range(x1,x2+1):
                #     for v in range(y1,y2+1):
                #         depth_val = depth_image[v,u]
                #         if(depth_val > 0 and mask[v,u]):
                #             [x,y,z] = rs.rs2_deproject_pixel_to_point(
                #                 depth_intrin, [u, v], depth_val / 1000.0)
                #             # print('ç‰©ä½“åæ ‡æ˜¯ï¼šx:',x,'y:',y,'z:',z)
                #             break

        except queue.Empty:
            # å½“é˜Ÿåˆ—ä¸ºç©ºæ—¶ï¼Œç»§ç»­å¾ªç¯è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
            pass
        if out_image:
            # æ˜¾ç¤ºç»“æœ
            cv2.imshow("output image", out_image)
            # ç»™ç³»ç»Ÿ 1ms æ—¶é—´åˆ·æ–°çª—å£
            cv2.waitKey(1)

            key = cv2.waitKey(1)& 0xFF
            # æŒ‰qæˆ–è€…ESCé€€å‡º
            if key == ord('q') or key == 27:
                break

if __name__ == "__main__":
        main()
