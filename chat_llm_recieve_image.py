import os
import sys
original_cwd = os.getcwd()
import time
import requests
import json
from prompt_build import get_inst_find,get_inst_figure,get_inst_plan,get_inst_chat,get_inst_see

import threading
import uvicorn
from fastapi import FastAPI, Request
import queue
import cv2
import torch
# çº¿ç¨‹å®‰å…¨é˜Ÿåˆ—ï¼ˆæ¨èï¼‰
stt_queue = queue.Queue(maxsize=5)
# æ¥æ”¶STTæœåŠ¡å™¨çš„æ¶ˆæ¯
app = FastAPI(title="Main STT Receiver")

@app.post("/v1")
async def receive_stt_text(request: Request):
    try:
        body = await request.json()  # â† å¿…é¡» awaitï¼
        text = body.get("text", "")
        if text and text.strip():
            stt_queue.put(text.strip())
            print(f"ğŸ“¨ ä¸»è¿›ç¨‹æ”¶åˆ°STT: {text}")
        return {"status": "ok"}
    except Exception as e:
        print(f"âŒ è§£æJSONå¤±è´¥: {e}")
        return {"status": "error", "message": str(e)}

def start_stt_receiver():
    """åœ¨åå°çº¿ç¨‹å¯åŠ¨HTTPæ¥æ”¶æœåŠ¡"""
    uvicorn.run(app, host="127.0.0.1", port=28184, log_level="warning")
    print("STT receiver started."," host:","127.0.0.1"," port:",28184)

# å¯åŠ¨æ¥æ”¶æœåŠ¡ï¼ˆdaemon=True ç¡®ä¿éšä¸»è¿›ç¨‹é€€å‡ºï¼‰
threading.Thread(target=start_stt_receiver, daemon=True).start()

# TTSå‘é€ç»™æœåŠ¡å™¨
class TTSAgent:
    def __init__(self, host_url):
        print(f"TTSAgent: host_url {host_url}")
        self.host_url = host_url

    def run(self, input_dict: dict) -> str:
        # ç›´æ¥ä¼ å…¥å­—å…¸ï¼Œä¸è¦åœ¨å¤–é¢è½¬å­—ç¬¦ä¸²
        try:
            # è¿™é‡Œçš„è·¯å¾„ç›´æ¥æŒ‡å‘ /v1ï¼Œä¸ä½¿ç”¨ 'exec'
            resp = requests.post(self.host_url, json=input_dict, timeout=5)
            if resp.status_code == 200:
                return "ok"
        except Exception as e:
            print(f"TTSAgent Error: {e}")
        return ""

tts = TTSAgent("http://127.0.0.1:28185/v1")

def tts_sound(tts_agent, text, lang, interrupt):
    # æ„é€ å­—å…¸
    input_dict = {"task": "text_to_speech", "lang": lang, "text": text, "interrupt": interrupt}
    print(f"ğŸ“¤ å‘é€ä¸­: {text}")

    # ç›´æ¥å‘é€
    tts_agent.run(input_dict)
    return 0

# åˆ‡æ¢åˆ°sam2å·¥ä½œç›®å½•ï¼ŒåŠ è½½sam
print('å¼€å§‹åŠ è½½sam2æ¨¡å‹')
start_time = time.time()
sam_path = original_cwd+'/camera/Grounded-SAM-2'
sys.path.append(sam_path)
from sam2.build_sam import build_sam2
from sam2.sam2_image_predictor import SAM2ImagePredictor
from PIL import Image
CHECKPOINT_SAM = os.path.join(sam_path,"sam2_checkpoints", "sam2.1_hiera_small.pt")
# CONFIG_SAMå‰é¢å¿…é¡»åŠ '/'
CONFIG_SAM = '/'+sam_path+"/sam2/configs/sam2.1/sam2.1_hiera_s.yaml"
# Load the model
sam2_model = build_sam2(CONFIG_SAM, CHECKPOINT_SAM, device="cuda")
sam2_model.half()
sam_predictor = SAM2ImagePredictor(sam2_model)
print(f"ğŸ‰ åŠ è½½SAMæ¨¡å‹è€—æ—¶: {time.time() - start_time:.2f} ç§’")


# å¯åŠ¨å¤§æ¨¡å‹å®¢æˆ·ç«¯
print('å¼€å§‹è¿æ¥å¤§æ¨¡å‹æœåŠ¡')
start_time= time.time()
import base64
from openai import OpenAI
client = OpenAI(base_url="http://192.168.20.49:8000/v1", api_key="EMPTY")
print(f"ğŸ‰ åŠ è½½å¤§æ¨¡å‹æœåŠ¡è€—æ—¶: {time.time() - start_time:.2f} ç§’")

import io
import numpy as np
# åˆæˆmessages
def input_messages_build(prompt, image=None):
    # prompt='å›¾ç‰‡ä¸­æœ‰ä»€ä¹ˆå†…å®¹'
    print(prompt)
    # æ„å»ºæ–‡æœ¬ä¿¡æ¯
    text_content = {
        "type": "text",
        "text": prompt
    }
    # print('image is:',image)
    if image is None:
        return [
        {
            "role": "user",
            "content": [text_content],
        }]
    else:
        # 1. åˆ›å»ºä¸€ä¸ªå­—èŠ‚æµå®¹å™¨
        buffer = io.BytesIO()
        # 2. å°† PIL å›¾åƒä¿å­˜åˆ°å®¹å™¨ä¸­ï¼ˆæŒ‡å®šæ ¼å¼ï¼‰
        image.save(buffer, format="PNG")
        # è½¬æ¢ä¸º base64
        image_b64 = base64.b64encode(buffer.getvalue()).decode("utf-8")
        # æ„å»ºå›¾åƒä¿¡æ¯
        image_contents = {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/png;base64,{image_b64}",
                        "min_pixels": 256 * 256,
                        "max_pixels": 2000 * 2000
                    },
                }
        return [
            {
                "role": "user",
                "content":   [text_content, image_contents],
            }]

import re
sentence_endings = r'[ã€‚ï¼ï¼Ÿ!?ï¼›;â€¦\n]'  # ä¸­è‹±æ–‡å¥å°¾ç¬¦å·
# å¾—åˆ°å›ç­”å¹¶è§£æå‡ºåŒ…å›´æ¡†
def get_bounding_box(model,text,image):
    width, height = image.size
    prompt = get_inst_find(text)
    messages = input_messages_build(prompt, image)
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=0.2,
        extra_body={"mm_processor_kwargs":{"fps": [1]}},
        stream=False,
    )
    # print(response)
    chunk = response.choices[0].message.content
    print("å›å¤ä¸ºï¼š",chunk)
    if chunk is None or chunk == '[]':
        # tts_sound(tts,"æœªæ‰¾åˆ°ç‰©ä½“","zh",True)
        entity = []
        return entity
    qwen3_entity = json.loads(chunk)
    [x1,y1,x2,y2]=qwen3_entity
    entity=[round(x1*width/1000), round(y1*height/1000),
            round(x2*width/1000), round(y2*height/1000)]
    print('åŒ…å›´æ¡†ä¸ºï¼š',entity)
    # tts_sound(tts,"å·²æ‰¾åˆ°ç‰©ä½“","zh",True)

    # entity = stream_message.strip('```json\n').strip('```')
    # print("entityï¼š", entity)
    return entity

# æ ¹æ®ç”¨æˆ·çš„è¯æå–å‡ºè¢«æ‹¿ç‰©ä½“ä¸æ”¾ç½®å®¹å™¨
def extract_objects(model,text):
    prompt = get_inst_figure(text)
    messages = input_messages_build(prompt)
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=0.2,
        extra_body={"mm_processor_kwargs":{"fps": [1]}},
        stream=False,
    )
    """å®Œæ•´è§£æå‡½æ•°ï¼Œå¤„ç†å„ç§å¯èƒ½çš„è¾“å‡ºæ ¼å¼"""
    raw_content = response.choices[0].message.content

    # æ–¹æ³•1: å°è¯•ç›´æ¥è§£æJSON
    try:
        clean_content = raw_content.strip()
        parsed = json.loads(clean_content)
        return {
            "è¢«æ‹¿ç‰©ä½“": parsed["è¢«æ‹¿ç‰©ä½“"],
            "æ”¾ç½®å®¹å™¨": parsed["æ”¾ç½®å®¹å™¨"]
        }
    except:
        pass

    # æ–¹æ³•2: å°è¯•æå–JSONç‰‡æ®µ
    try:
        json_match = re.search(r'\{[^{}]*\}', raw_content)
        if json_match:
            parsed = json.loads(json_match.group(0))
            return {
                "è¢«æ‹¿ç‰©ä½“": parsed["è¢«æ‹¿ç‰©ä½“"],
                "æ”¾ç½®å®¹å™¨": parsed["æ”¾ç½®å®¹å™¨"]
            }
    except:
        pass

    # æ–¹æ³•3: æ­£åˆ™åŒ¹é…å…³é”®å­—æ®µ
    main_obj = re.search(r'è¢«æ‹¿ç‰©ä½“\s*[:ï¼š]\s*([^\s,]+)', raw_content)
    target_obj = re.search(r'æ”¾ç½®å®¹å™¨\s*[:ï¼š]\s*([^\s,]+)', raw_content)

    return {
        "è¢«æ‹¿ç‰©ä½“": main_obj.group(1) if main_obj else "",
        "æ”¾ç½®å®¹å™¨": target_obj.group(1) if target_obj else ""
    }
# æ ¹æ®ç”¨æˆ·çš„è¯è¿›è¡Œå¯¹è¯
def chat_with_llm(model,text,image=None):
    start_time = time.time()
    first = True
    if image is None:
        prompt = get_inst_chat(text)
        messages = input_messages_build(prompt)
    else:
        prompt = get_inst_see(text)
        messages = input_messages_build(prompt, image)
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=0.2,
        extra_body={"mm_processor_kwargs":{"fps": [1]}},
        stream=True,
    )
    buffer = ""
    for chunk in response:
        if chunk.choices and chunk.choices[0].delta.content:
            token = chunk.choices[0].delta.content
            buffer += token
            # æ£€æŸ¥æ˜¯å¦é‡åˆ°å®Œæ•´å¥å­ç»“å°¾
            if re.search(sentence_endings, token):
                # å»æ‰æœ«å°¾ç©ºç™½
                sentence = buffer.strip()
                if sentence:
                    tts_sound(tts,sentence,"zh", first)
                    if first:
                        print(f"å†æ¬¡åˆ¤æ–­è€—æ—¶: {time.time()-start_time}")
                        first = False
                    buffer = ""  # æ¸…ç©ºç¼“å†²åŒº

    # å¤„ç†æœ€åä¸€å¥ï¼ˆå¦‚æœæ²¡ä»¥å¥å·ç»“å°¾ï¼‰
    if buffer.strip():
        sentence = buffer.strip()
        tts_sound(tts,sentence,"zh",False)

# å›¾åƒçº¿ç¨‹å®‰å…¨é˜Ÿåˆ—ï¼ˆæ¨èï¼‰
img_queue = queue.Queue(maxsize=1)
image_app = FastAPI(title="Image Receiver")
@image_app.post("/api/process_image")
async def receive_image(request: Request):
    # 1. è§£æJSONè¯·æ±‚ä½“ï¼ˆæ›¿ä»£ç»“æ„ä½“ï¼Œç›´æ¥è¯»å–ï¼‰
    req_body = await request.json()

    # 2. æ ¡éªŒå¿…å¡«å­—æ®µæ˜¯å¦å­˜åœ¨
    required_fields = ["image", "width", "height"]

    # 3. æå–å­—æ®µå¹¶æ ¡éªŒç±»å‹ï¼ˆwidth/heightéœ€ä¸ºæ•°å­—ï¼‰
    image_data = req_body["image"]
    width = req_body["width"]
    height = req_body["height"]

    # 5. è§£ç base64ï¼ˆå»é™¤å‰ç¼€åï¼‰
    image_base64 = image_data.split(',')[1]  # å»æ‰å‰ç¼€éƒ¨åˆ†
    image_bytes = base64.b64decode(image_base64)

    # 6. éªŒè¯PNGå›¾åƒæœ‰æ•ˆæ€§
    img = Image.open(io.BytesIO(image_bytes)).convert('RGB')
    # img_queue.put(img)
    # å¯é€‰ï¼šä¿å­˜æ¥æ”¶åˆ°çš„å›¾åƒ
    img.save('received_image.png', format="PNG")
    # æ ¡éªŒä¼ å…¥çš„å®½é«˜å’Œå®é™…å›¾åƒå®½é«˜æ˜¯å¦ä¸€è‡´ï¼ˆå¯é€‰ï¼Œæ ¹æ®GUIéœ€æ±‚ï¼‰
    actual_width, actual_height = img.size
    if int(width) != actual_width or int(height) != actual_height:
        print(f"âš ï¸ ä¼ å…¥å®½é«˜({width}x{height})ä¸å®é™…å›¾åƒå®½é«˜({actual_width}x{actual_height})ä¸ä¸€è‡´")

    # 7. æ‰“å°è°ƒè¯•ä¿¡æ¯ï¼ˆç¡®è®¤æ¥æ”¶æˆåŠŸï¼‰
    client_ip = request.client.host
    print(f"\n=== æˆåŠŸæ¥æ”¶GUIå‘æ¥çš„å›¾åƒ ===")
    print(f"å®¢æˆ·ç«¯IPï¼š{client_ip}")
    print(f"ä¼ å…¥å®½é«˜ï¼š{width}x{height}")
    print(f"å®é™…å®½é«˜ï¼š{actual_width}x{actual_height}")

def start_img_receiver():
    """åœ¨åå°çº¿ç¨‹å¯åŠ¨HTTPæ¥æ”¶æœåŠ¡"""
    uvicorn.run(image_app, host="0.0.0.0", port=50056, log_level="warning")
    print("Image receiver started."," host:","0.0.0.0"," port:",50056)
# å¯åŠ¨æ¥æ”¶æœåŠ¡ï¼ˆdaemon=True ç¡®ä¿éšä¸»è¿›ç¨‹é€€å‡ºï¼‰
threading.Thread(target=start_img_receiver, daemon=True).start()

def send_mask_to_gui(mask_image, gui):
    """
    å°† mask å‘é€åˆ° GUI

    Args:
        mask_image: PIL Image å¯¹è±¡ï¼ˆç°åº¦æ¨¡å¼ï¼ŒL modeï¼‰
        gui_url: GUI æœåŠ¡å™¨åœ°å€
    """
    gui_url="http://192.168.20.29:8000"
    # è½¬æ¢ä¸º Base64
    buffered = io.BytesIO()
    mask_image.save(buffered, format="PNG")
    mask_base64 = base64.b64encode(buffered.getvalue()).decode('utf-8')
    mask_data = f"data:image/png;base64,{mask_base64}"

    # å‘é€è¯·æ±‚
    response = requests.post(
        f"{gui_url}{gui}",
        headers={'Content-Type': 'application/json'},
        json={'mask_data': mask_data},
        timeout=10
    )

    if response.status_code == 200:
        result = response.json()
        if result.get('success'):
            print("Mask sent successfully")
        else:
            print(f"Failed: {result.get('message')}")
    else:
        print(f"HTTP Error: {response.status_code}")


def main():
    # cv2.namedWindow("output image", cv2.WINDOW_AUTOSIZE)
    out_image = None
    while True:

        try:
            text = stt_queue.get_nowait()
            start_time = time.time()
            prompt = get_inst_plan(text)
            messages = input_messages_build(prompt)
            response = client.chat.completions.create(
                model='Qwen3-VL-8B-Instruct',
                messages=messages,
                temperature=0.2,
                extra_body={"mm_processor_kwargs":{"fps": [1]}},
                stream=False,
            )
            choices = response.choices[0].message.content
            print('é€‰æ‹©æ“ä½œè€—æ—¶ï¼š',time.time()-start_time,'ç§’')
            print("é€‰æ‹©çš„æ“ä½œç±»å‹ï¼š", choices)
            image = Image.open('received_image.png').convert('RGB')
            if choices=='C':
                chat_with_llm('Qwen3-VL-8B-Instruct',text)
                continue
            elif choices=='S':
                chat_with_llm('Qwen3-VL-8B-Instruct',text,image)
                continue
            # image = send_signal_get_image()
            # image = img_queue.get_nowait()
            # image.save('received_image.png', format="PNG")
            result = extract_objects('Qwen3-VL-8B-Instruct',text)
            print('è¾“å…¥è¯­å¥åˆ†å‰²ç»“æœä¸º',result)
            # out_image = image.copy()
            text1 = result['è¢«æ‹¿ç‰©ä½“']
            start_time = time.time()
            # è·å–åŒ…å›´æ¡†
            input_box = get_bounding_box('Qwen3-VL-8B-Instruct',text1,image)
            print("åŒ…å›´æ¡†æ—¶é—´ä¸º",time.time()-start_time)
            if input_box!=[]:
                start_time = time.time()
                x1, y1, x2, y2 = input_box
                # -------------------------- SAM3æ¨ç† --------------------------
                with torch.inference_mode(), torch.autocast(device_type="cuda", dtype=torch.float16):
                    inference_state = sam_predictor.set_image(image)
                    masks, scores, _ = sam_predictor.predict(
                        box=input_box,
                        multimask_output=False  # å•maskæ›´é«˜æ•ˆ
                    )
                print("æ¡†æ©ç é¢„æµ‹æ—¶é—´ä¸º",time.time()-start_time)
                mask = masks[np.argmax(scores)]
                # æ©ç è½¬äºŒå€¼å›¾
                mask_np = mask.astype(np.uint8) * 255

                print(f"1. å½¢çŠ¶ï¼ˆshapeï¼‰ï¼š{mask_np.shape} â†’ (é«˜åº¦, å®½åº¦)ï¼ˆäºŒå€¼æ©ç æ— é€šé“ï¼‰")
                img_np = np.array(image)
                # 2. è½¬æ¢é€šé“é¡ºåºï¼šRGB â†’ BGRï¼ˆOpenCVæ˜¾ç¤ºçš„æ ¸å¿ƒè¦æ±‚ï¼‰
                img_cv2 = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)
                print(f"1. å½¢çŠ¶ï¼ˆshapeï¼‰ï¼š{img_cv2.shape} â†’ (é«˜åº¦, å®½åº¦, é€šé“æ•°)ï¼ˆBGRé€šé“ï¼‰")
                cv2.rectangle(img_cv2, (x1, y1), (x2, y2), (0, 255, 0), 2)
                out_image = img_cv2
                mask_pil = Image.fromarray(mask_np)
                try:
                    send_mask_to_gui(mask_pil,'/external/receive_mask')
                except:
                    pass
            else:
                sentence = 'æœªæ‰¾åˆ°'+text1
                tts_sound(tts,sentence,"zh", False)
            # print("å¯è§†åŒ–")
            # cv2.namedWindow("detection window", cv2.WINDOW_AUTOSIZE)
                # å¼ºåˆ¶çª—å£ç½®é¡¶ï¼ˆæŸäº›Linuxæ¡Œé¢ç¯å¢ƒæ”¯æŒï¼‰
            #     cv2.setWindowProperty("detection window", cv2.WND_PROP_TOPMOST, 1)
            #     cv2.imshow("detection window", mask_np)
            #     cv2.waitKey(100)

            # if out_image is not None:
            #     cv2.imshow("output image", out_image)
            #     cv2.waitKey(100)
            # continue
            text2 = result['æ”¾ç½®å®¹å™¨']
            start_time = time.time()
            # è·å–åŒ…å›´æ¡†
            input_box = get_bounding_box('Qwen3-VL-8B-Instruct',text2,image)
            print("åŒ…å›´æ¡†æ—¶é—´ä¸º",time.time()-start_time)
            if input_box!=[]:
                start_time = time.time()
                x1, y1, x2, y2 = input_box
                # -------------------------- SAM3æ¨ç† --------------------------
                with torch.inference_mode(), torch.autocast(device_type="cuda", dtype=torch.float16):
                    inference_state = sam_predictor.set_image(image)
                    masks, scores, _ = sam_predictor.predict(
                        box=input_box,
                        multimask_output=False  # å•maskæ›´é«˜æ•ˆ
                    )
                print("æ¡†æ©ç é¢„æµ‹æ—¶é—´ä¸º",time.time()-start_time)
                mask = masks[np.argmax(scores)]
                # æ©ç è½¬äºŒå€¼å›¾
                mask_np = mask.astype(np.uint8) * 255
                mask_pil = Image.fromarray(mask_np)
                try:
                    send_mask_to_gui(mask_pil,'/external/receive_place_target')
                except:
                    pass
            else:
                sentence = 'æœªæ‰¾åˆ°'+text2
                tts_sound(tts,sentence,"zh", False)
                # -------------------------- å…³é”®æ­¥éª¤ï¼šæ˜¾ç¤ºå¹¶å¼ºåˆ¶åˆ·æ–° --------------------------

                # è·å¾—è¯¥ç‰©ä½“çš„xyzåæ ‡ï¼ˆåŒ…å›´æ¡†å†…æ©ç çš„åƒç´ ç‚¹å¯¹åº”çš„xyzæ±‚å¹³å‡ï¼‰
                # for u in range(x1,x2+1):
                #     for v in range(y1,y2+1):
                #         depth_val = depth_image[v,u]
                #         if(depth_val > 0 and mask[v,u]):
                #             [x,y,z] = rs.rs2_deproject_pixel_to_point(
                #                 depth_intrin, [u, v], depth_val / 1000.0)
                #             # print('ç‰©ä½“åæ ‡æ˜¯ï¼šx:',x,'y:',y,'z:',z)
                #             break

        except queue.Empty:
            # å½“é˜Ÿåˆ—ä¸ºç©ºæ—¶ï¼Œç»§ç»­å¾ªç¯è€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
            pass

        #     key = cv2.waitKey(1)& 0xFF
        #     # æŒ‰qæˆ–è€…ESCé€€å‡º
        #     if key == ord('q') or key == 27:
        #         break

if __name__ == "__main__":
        main()
